---
title: "Compulsory Exercise 2: Prediction of Alzheimers Disease from Longitudinal MRI Data"
author:
- Chester Henry Charlton
- Kasper Eikeland
- Tinus Garshol
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: \usepackage{amsmath}
output:
  html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  #pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: "This is the place for your abstract (max 350 words)"
---
  
```{r setup, include=FALSE}
library("knitr")
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("rmarkdown")
library("dplyr")
library("GGally")
library("tidyr")
library("formatR")
```

```{r}
#setwd("/Users/chcharlton/NTNU/statLearning/compulsory_alzheimers")
setwd("/Users/kaspe/Documents/NTNU/23V/TMA4268_Statistical_Learning/Projects/tma4268_alzheimers")
```


<!--  Henry's initial work here -->


<!--  Kasper's initial work here. -->
```{r}
cross_sectional.df <- read.csv("oasis_cross-sectional.csv")

# Patient is demented if CDR is > 0
cs.mutated <- cross_sectional.df %>%
  mutate(Demented = as.factor(CDR>0),
         M.F = as.factor(M.F)) %>%
  dplyr::select(c(M.F, eTIV, nWBV, ASF, Age, Demented)) %>%
  na.omit()
  

longitudinal.df <- read.csv("oasis_longitudinal.csv")

longitudinal_deltas <- longitudinal.df %>% 
  group_by(Subject.ID) %>%
  summarise(dMMSE = (last(MMSE) - first(MMSE)) / last(Visit), 
            deTIV = (last(eTIV) - first(eTIV)) / last(Visit),
            dnWBV = (last(nWBV) - first(nWBV)) / last(Visit),
            dASF = (last(ASF) - first(ASF)) / last(Visit),
            dAge = (last(Age) - first(Age)) / last(Visit))

baseline_with_deltas <- longitudinal.df %>%
  filter(Visit == 1) %>%
  left_join(longitudinal_deltas, by = c("Subject.ID")) %>%
  mutate(Demented = as.factor(Group != "Nondemented"), # Those who converted became demented
         M.F = as.factor(M.F)) %>% 
  dplyr::select(-c(Subject.ID, MRI.ID, Group, Visit, MR.Delay, Hand, CDR, SES)) 
  
set.seed(1337)

# Longitudinal data
l.train.indeces <- sample(1:nrow(baseline_with_deltas), floor(.75 * nrow(baseline_with_deltas)))
longitudinal.train <- na.omit(baseline_with_deltas[l.train.indeces,])
longitudinal.test <- na.omit(baseline_with_deltas[-l.train.indeces,])

# Cross-sectional
cs.train.indeces <- sample(1:nrow(cs.mutated), floor(.75 * nrow(cs.mutated)))
cs.train <- na.omit(cs.mutated[cs.train.indeces,])
cs.test <- na.omit(cs.mutated[-cs.train.indeces,])
```

```{r}
library("tree")
library("gbm")
library("randomForest")
```


```{r}
# Longitudinal
# Classification tree
set.seed(1337)
longitudinal.tree <- tree(Demented ~., longitudinal.train)
summary(longitudinal.tree)
plot(longitudinal.tree)
text(longitudinal.tree, pretty = 0)

# Test predictions
pred.tree <- predict(longitudinal.tree, newdata=longitudinal.test, type = "class")
table(pred = pred.tree, truth = longitudinal.test$Demented) # 0.7837838

# Pruning
longitudinal.cv <- cv.tree(longitudinal.tree, FUN = prune.misclass)
longitudinal.cv
# Seems 7 leaves give the minimum amount of misclassifications of the training data
par (mfrow = c(1, 2))
plot (longitudinal.cv$size , longitudinal.cv$dev, type = "b")
plot (longitudinal.cv$k, longitudinal.cv$dev, type = "b")

longitudinal.prune <- prune.misclass(longitudinal.tree , best = 7)
plot(longitudinal.prune)
text(longitudinal.prune, pretty = 0)

pred.prune <- predict(longitudinal.prune, newdata=longitudinal.test, type = "class")
table(pred = pred.prune, truth = longitudinal.test$Demented) # 0.7837838

# Lower accuracy, more interperateable since fewer decisions to make
```

```{r}
# Cross sectional
# Classification tree
set.seed(1337)
cs.tree <- tree(Demented ~., cs.train)
summary(cs.tree)
plot(cs.tree)
text(cs.tree, pretty = 0)

# Test predictions
pred.tree <- predict(cs.tree, newdata=cs.test, type = "class")
table(pred = pred.tree, truth = cs.test$Demented) # 0.7837838

# Pruning
cs.cv <- cv.tree(cs.tree, FUN = prune.misclass)
cs.cv
# Seems 7 leaves give the minimum amount of misclassifications of the training data
par (mfrow = c(1, 2))
plot (cs.cv$size , cs.cv$dev, type = "b")
plot (cs.cv$k, cs.cv$dev, type = "b")

cs.prune <- prune.misclass(cs.tree , best = 12)
plot(cs.prune)
text(cs.prune, pretty = 0)

pred.prune <- predict(cs.prune, newdata=cs.test, type = "class")
table(pred = pred.prune, truth = cs.test$Demented) # 0.7837838

# Higher accuracy, more interperateable since fewer decisions to make
```

```{r}
# Bagging
set.seed(1337)
longitudinal.bag <- randomForest(Demented ~., longitudinal.train, mtry = 12, importance = TRUE)
summary(longitudinal.bag)
plot(longitudinal.bag)
yhat.bag <- predict(longitudinal.bag, newdata = longitudinal.test, type = "class")
table(pred = yhat.bag, truth = longitudinal.test$Demented) # 0.8648649

# Importance of variables 
varImpPlot(longitudinal.bag)
```


```{r}
# Random forest
set.seed(1337)
longitudinal.rf <- randomForest(Demented ~., longitudinal.train, importance = TRUE)
summary(longitudinal.rf)
plot(longitudinal.rf)
yhat.rf <- predict(longitudinal.rf, newdata = longitudinal.test, type = "class")
table(pred = yhat.rf, truth = longitudinal.test$Demented) # 0.8108108

# Importance
varImpPlot(longitudinal.rf)
# dAge seems to be important??
```


```{r}
# Boosting
set.seed(1337)
longitudinal.train.boost <- mutate(longitudinal.train, Demented = ifelse(Demented == "FALSE", 0,1))
longitudinal.boost <- gbm(Demented ~., data = longitudinal.train.boost, 
                          distribution = "bernoulli", n.trees = 5000, interaction.depth = 2)
summary(longitudinal.boost)
longitudinal.test.boost <- mutate(longitudinal.test, Demented = ifelse(Demented == "FALSE", 0,1))
yhat.boost <- predict(longitudinal.boost, newdata = longitudinal.test.boost, n.trees = 5000, type = "response")
yhat.boost <- ifelse(yhat.boost > 0.5, 1,0)
table(pred = yhat.boost, truth = longitudinal.test.boost$Demented) # 0.8648649
```

<!--  Tinus' initial work here. -->



## Introduction: Scope and purpose of your project


## Descriptive data analysis/statistics


## Methods


## Results and interpretation


## Summary
