---
title: "Compulsory Exercise 2: Prediction of Alzheimers Disease from Longitudinal MRI Data"
author:
- Chester Henry Charlton
- Kasper Eikeland
- Tinus Garshol
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: \usepackage{amsmath}
output:
  html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  #pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: "This is the place for your abstract (max 350 words)"
---
  
```{r setup, include=FALSE}
library("knitr")
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("rmarkdown")
library("dplyr")
library("GGally")
library("tidyr")
library("formatR")
```

```{r}
#setwd("/Users/chcharlton/NTNU/statLearning/compulsory_alzheimers")
setwd("/Users/kaspe/Documents/NTNU/23V/TMA4268_Statistical_Learning/Projects/tma4268_alzheimers")
```


<!--  Henry's initial work here -->


<!--  Kasper's initial work here. -->
```{r}
cross_sectional.df <- read.csv("oasis_cross-sectional.csv")

# Patient is demented if CDR is > 0
cs.mutated <- cross_sectional.df %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, CDR)) %>%
  na.omit() %>%
  mutate(Demented = as.factor(CDR>0),
         M.F = as.factor(M.F)) %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, Demented))


longitudinal.df <- read.csv("oasis_longitudinal.csv")

longitudinal_deltas <- longitudinal.df %>% 
  group_by(Subject.ID) %>%
  summarise(dMMSE = (last(MMSE) - first(MMSE)) / last(Visit), 
            deTIV = (last(eTIV) - first(eTIV)) / last(Visit),
            dnWBV = (last(nWBV) - first(nWBV)) / last(Visit),
            dASF = (last(ASF) - first(ASF)) / last(Visit),
            dAge = (last(Age) - first(Age)) / last(Visit))

baseline_with_deltas <- longitudinal.df %>%
  filter(Visit == 1) %>%
  left_join(longitudinal_deltas, by = c("Subject.ID")) %>%
  mutate(Demented = as.factor(Group != "Nondemented"), # Those who converted became demented
         M.F = as.factor(M.F)) %>% 
  dplyr::select(-c(Subject.ID, MRI.ID, Group, Visit, MR.Delay, Hand, CDR, SES)) 
  
set.seed(1337)

# Longitudinal data
l.train.indeces <- sample(1:nrow(baseline_with_deltas), floor(.75 * nrow(baseline_with_deltas)))
longitudinal.train <- na.omit(baseline_with_deltas[l.train.indeces,])
longitudinal.test <- na.omit(baseline_with_deltas[-l.train.indeces,])

# Cross-sectional
cs.train.indeces <- sample(1:nrow(cs.mutated), floor(.75 * nrow(cs.mutated)))
cs.train <- na.omit(cs.mutated[cs.train.indeces,])
cs.test <- na.omit(cs.mutated[-cs.train.indeces,])
```

```{r}
library("tree")
library("gbm")
library("randomForest")
```

### Methods

We consider three different classifiers, Logistic Regression, Random Forests and Support Vector machines. We first introduce and describe the classifiers, then describe the methods used for performance evaluation and model selection. 

#### Logistic Regression

#### Random Forests

Random Forests is a tree ensemble method that can be used for both regression and classification. Each tree is trained on a bootstrap sample of the training data. The splits are made using a random subset of $m<p$ predictors. This decorrelates the trees and presents an imporvement over bagging thereof [cite ISLR]. For each tree we perform a classification on the Out-Of-Bag (OOB) data, the training observations not in the bootstrap sample the given tree was trained on. In this sense, each tree votes for a given class for each OOB observation. We then aggregate all votes and the forest predicts the class for each observation in the training data as the one which got the majority of the votes.


For each tree we calculate the Out-Of-Bag (OOB) error estimate, the misclassification rate calculated on the training observations not in the bootstrap sample the tree was trained on. 

Strengths/weaknessess: interpretability of classification trees versus predictive accuracy og random forests


#### Support vector machines

#### Model Selection



### Results


At the cost of interpretability we use random forests for classification instead of regular classification trees. 

```{r}
set.seed(1337)

p <- ncol(longitudinal.train) - 1 # number of predictors
N <- nrow(longitudinal.train) # number of training observations

trainx <- select(longitudinal.train, -Demented)
trainy <- longitudinal.train$Demented

rf <- randomForest(Demented ~., data = longitudinal.train, ntree = 5000, mtry = sqrt(p), importance = TRUE)
rf

rf.cv <- rfcv(trainx, trainy, cv.fold = 10)
with(rf.cv, plot(n.var, error.cv, log="x", type="o", lwd=2))

varImpPlot(rf)

preds <- predict(rf)
train.spec <- sum()

```




```{r}
# Boosting, longitudinal
set.seed(1337)

# gbm wants response in {0,1}
longitudinal.train.boost <- mutate(longitudinal.train, Demented = ifelse(Demented == "FALSE", 0,1))
```

### Boosted classification tree

Here we a classification tree by boosting. This is done using the ´gbm´ package. To choose the number of trees to use we employ 10-fold cross-validation though the function itself. To choose the interaction depth we examine four different models and choose the model with lowest overall cross-validation error. We use a shrinkage of 0.1 for all models. 

```{r}
gbm1 <- gbm(Demented ~., data = longitudinal.train.boost, distribution = "bernoulli" ,n.trees = 5000, interaction.depth = 1, shrinkage = 0.1, cv.folds = 10, n.cores = 6)
gbm2 <- gbm(Demented ~., data = longitudinal.train.boost, distribution = "bernoulli" ,n.trees = 5000, interaction.depth = 2, shrinkage = 0.1, cv.folds = 10, n.cores = 6)
gbm3 <- gbm(Demented ~., data = longitudinal.train.boost, distribution = "bernoulli" ,n.trees = 5000, interaction.depth = 3, shrinkage = 0.1, cv.folds = 10, n.cores = 6)
gbm4 <- gbm(Demented ~., data = longitudinal.train.boost, distribution = "bernoulli" ,n.trees = 5000, interaction.depth = 4, shrinkage = 0.1, cv.folds = 10, n.cores = 6)

iters = 1:100

cv.df <- data.frame(iteration = iters, 
                    cv1 = gbm1$cv.error[iters], cv2 = gbm2$cv.error[iters], 
                    cv3 = gbm3$cv.error[iters], cv4 = gbm4$cv.error[iters])

ggplot(cv.df, aes(x=iteration)) + 
  geom_line(aes(y = cv1, color = "d=1")) + 
  geom_line(aes(y = cv2, color = "d=2")) +
  geom_line(aes(y = cv3, color = "d=3")) +
  geom_line(aes(y = cv4, color = "d=4")) +
  geom_point(aes(x = which.min(cv1), y = min(cv1))) +
  geom_point(aes(x = which.min(cv2), y = min(cv2))) +
  geom_point(aes(x = which.min(cv3), y = min(cv3))) +
  geom_point(aes(x = which.min(cv4), y = min(cv4))) +
  labs(x = "Iteration", y = "Cross-validation error") + 
  theme_minimal()
```

From the above plot we see that an interaction depth of 2 gives the minimum CV-error with `{r} which.min(cv.df$cv2)` iterations. Thus we use an interaction depth of 2 with 22 trees and a shrinkage of 0.1 for the boosted classification tree. 




### Feature selection sensitivity test
Quick test to see if the feature selection is sensitive to the choice of parameters.
```{r}
# Feature selection sensitivity test
library("stepgbm")

trainx <- select(longitudinal.train.boost, -Demented)
trainy <- longitudinal.train.boost$Demented

# backwards feature selection
backwards.gbm <- function(trainx, trainy, B, d, shrinkage, folds) {
  set.seed(1337)
  p <- ncol(trainx)
  # Step 1
  M_k_list <- data.frame(predictors = rep(0, p), misclass_rate = rep(0, p), 
                         cv_error = rep(0,p), iterations = rep(0,p), n.pred = c(1:p)) # only save down to d predictors?
  
  M_k_list$predictors[p] <- list(1:p) # save all predictors
  
  predictors_k <- c(1:p) # start with all predictors
  
  M_k_tmp <- gbm(trainy ~ ., data = select(trainx, predictors_k),
                     distribution = "bernoulli", n.trees = B, interaction.depth = d, 
                     shrinkage = shrinkage, cv.folds = folds) # fit full model with all predictors
  
  M_k_list$misclass_rate[p] <- mean(M_k_tmp$train.error) # save average training error from gbm object
  M_k_list$predictors[p] <- list(predictors_k) # save corresponding predictors
  M_k_list$iterations[p] <- gbm.perf(M_k_tmp, plot.it = FALSE, method = "cv") # save best number of trees
  M_k_list$cv_error[p] <- min(M_k_tmp$cv.error) # save min cv-error from full model
  
  # Step 2
  # Consider all k models that contain all but one of the predictors in M_k for a total of k-1 predictors

  for (k in ncol(trainx):(d+1)) { # iterate backwards, minimum d predictors
  
    results_k.df <- data.frame(predictors = I(rep(list(rep(0,k-1)), k)), misclass_rate = rep(0,k),
                               cv_error = rep(0, k), iterations = rep(0, k)) # k rows, 3 columns, first column is which predictors
    
    for (j in 1:k) { # iterate through predictors_k
      current_predictors <- predictors_k[-j]  # exclude jth predictor
      
      M_k_tmp <- gbm(trainy ~ ., data = select(trainx, current_predictors),
                     distribution = "bernoulli", n.trees = B, interaction.depth = d, 
                     shrinkage = shrinkage) # fit model with current selection of predictors
      
      results_k.df$misclass_rate[j] <- mean(M_k_tmp$train.error) # save average training error from gbm object
      results_k.df$predictors[j] <- list(current_predictors) # save corresponding predictors
      }
    # maybe use mean training loss?
    best.model_k <- which.min(results_k.df$misclass_rate)# select model with minimum mean train loss - should we use a different measure?
    
    predictors_k <- unlist(results_k.df$predictors[best.model_k]) # get predictors from model M_k, vector of indeces
    
    M_k_list$misclass_rate[k-1] <- results_k.df$misclass_rate[best.model_k] # save the mean training loss
    M_k_list$predictors[k-1] <- list(predictors_k) # save which predictors give min mean training loss for k predictors
    }
  
  # refit selected models and do cv except last one, already done it 
  for (k in d:(p-1)) { # only consider models with at least d predictors
    print(k)
    predictors <- unlist(M_k_list$predictors[k])
    model <- gbm(trainy ~ ., data = select(trainx, predictors),
                     distribution = "bernoulli", n.trees = B, interaction.depth = d, 
                     shrinkage = shrinkage, cv.folds = folds) # fit model with best combination of k predictors
    
    M_k_list$iterations[k] <- gbm.perf(model, plot.it = FALSE, method = "cv") # save best number of trees
    M_k_list$cv_error[k] <- model$cv.error[M_k_list$iterations[k]] # save cv-error from full model
    
  }
  return(M_k_list)
} 

set.seed(1337)
best_subset1 <- backwards.gbm(trainx, trainy, B = 1000, d = 2, shrinkage = 0.1, folds = 10)

ggplot(best_subset1, aes(x=n.pred)) + 
  geom_line(aes(y = cv_error, color = "d=2")) +
  geom_point(aes(x = which.min(cv_error), y = min(cv_error))) +
    labs(x = "Number of predictors", y = "Cross-validation error") + 
  theme_minimal()
```




### Feature selection
We fix a set of parameters and find the best subset of features given these parameter values, chosen with AIC.

```{r}

```


### Paramter tuning
Now we tune the parameters to the chosen subset of predictors with K-fold cross validation. 

```{r}

```

This is all like a greedy algorithm for finding the best model out of all possible models by reducing the search space



Now that we have the best boosted model out of all the 
### CV each method

### Result
```{r}
longitudinal.test.boost <- mutate(longitudinal.test, Demented = ifelse(Demented == "FALSE", 0,1))

yhat.boost <- predict(longitudinal.boost, newdata = longitudinal.test.boost, 
                      n.trees = 5000, type = "response")
yhat.boost <- ifelse(yhat.boost > 0.5, 1,0)
table(pred = yhat.boost, truth = longitudinal.test.boost$Demented) # 0.8648649

```



<!--  Tinus' initial work here. -->



## Introduction: Scope and purpose of your project


## Descriptive data analysis/statistics


## Methods


## Results and interpretation


## Summary
