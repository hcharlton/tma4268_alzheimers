---
title: "Compulsory Exercise 2: Prediction of Alzheimers Disease from Longitudinal MRI Data"
author:
- Chester Henry Charlton
- Kasper Eikeland
- Tinus Garshol
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: \usepackage{amsmath}
output:
  html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  #pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: "This is the place for your abstract (max 350 words)"
---
  
```{r setup, include=FALSE}
library("knitr")
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("rmarkdown")
library("dplyr")
library("GGally")
library("tidyr")
library("formatR")
```

```{r}
#setwd("/Users/chcharlton/NTNU/statLearning/compulsory_alzheimers")
setwd("/Users/kaspe/Documents/NTNU/23V/TMA4268_Statistical_Learning/Projects/tma4268_alzheimers")
```


<!--  Henry's initial work here -->


<!--  Kasper's initial work here. -->
```{r}
cross_sectional.df <- read.csv("oasis_cross-sectional.csv")

# Patient is demented if CDR is > 0
cs.mutated <- cross_sectional.df %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, CDR)) %>%
  na.omit() %>%
  mutate(Demented = as.factor(CDR>0),
         M.F = as.factor(M.F)) %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, Demented))


longitudinal.df <- read.csv("oasis_longitudinal.csv")

longitudinal_deltas <- longitudinal.df %>% 
  group_by(Subject.ID) %>%
  summarise(dMMSE = (last(MMSE) - first(MMSE)) / last(Visit), 
            deTIV = (last(eTIV) - first(eTIV)) / last(Visit),
            dnWBV = (last(nWBV) - first(nWBV)) / last(Visit),
            dASF = (last(ASF) - first(ASF)) / last(Visit),
            dAge = (last(Age) - first(Age)) / last(Visit))

baseline_with_deltas <- longitudinal.df %>%
  filter(Visit == 1) %>%
  left_join(longitudinal_deltas, by = c("Subject.ID")) %>%
  mutate(Demented = as.factor(Group != "Nondemented"), # Those who converted became demented
         M.F = as.factor(M.F)) %>% 
  dplyr::select(-c(Subject.ID, MRI.ID, Group, Visit, MR.Delay, Hand, CDR, SES)) 
  
set.seed(1337)

# Longitudinal data
l.train.indeces <- sample(1:nrow(baseline_with_deltas), floor(.75 * nrow(baseline_with_deltas)))
longitudinal.train <- na.omit(baseline_with_deltas[l.train.indeces,])
longitudinal.test <- na.omit(baseline_with_deltas[-l.train.indeces,])

# Cross-sectional
cs.train.indeces <- sample(1:nrow(cs.mutated), floor(.75 * nrow(cs.mutated)))
cs.train <- na.omit(cs.mutated[cs.train.indeces,])
cs.test <- na.omit(cs.mutated[-cs.train.indeces,])
```

```{r}
library("randomForest")
```

### Methods

We consider three different classifiers, Logistic Regression, Random Forests and Support Vector machines. We first introduce and describe the classifiers, then describe the methods used for performance evaluation and model selection. 

#### Logistic Regression

#### Random Forests

We use Random Forests as a classifier as described in [ISLR2]. Random Forests is a tree ensemble method that can be used for both regression and classification. Each tree is trained on a bootstrap sample of the training data. The splits are made using a random subset of $m<p$ predictors. This decorrelates the trees and presents an imporvement over bagging thereof [cite ISLR]. For each tree we perform a classification on the Out-Of-Bag (OOB) observations, the training observations not in the bootstrap sample the given tree was trained on. In this sense, each tree votes for a given class for each OOB observation. We then aggregate all votes and the forest predicts the class for each observation in the training data as the one which got the majority of the votes. 

This method of growing decision trees and making predictions offers for higher predictive accuracy at the cost of lower interpretability compared to standard classification trees. For our purposes we value the predictive accuracy higher. However, a case can be made for more interpretability as such a decision tree could in theory be used as an easy-to-follow clinical test for deciding whether or not a patient has AD based on similar data. Still, if the predictive accuracy of such a test is not high enough, its value as a clinical test is lowered.

We implement Random Forests through the R package `randomForest`.


#### Support vector machines


#### Model Selection

To select a model we need to consider all possible models and make a decision based on some measure of goodness-of-fit. For this measure we use the misclassification rate to compute a cross-validated error estimate for one model of each classifier. To select one model for of each classifier for comparison, we use different methods for each classifier. A graphical representation of our model selection algorithm is shown below. We have chosen to initially only compare models of the same classifier as this seperates the search space and reduces the amount of computations considerably.

*Insert graphic here*

##### Logistic

##### Random Forests

Random Forests can be tuned by two parameters, the number of trees $B$ and the amount of predictors to consider for each split $m$. The number of trees used to fit a random forest does not impact the bias-variance tradeoff as long as its high enough as high $B$ does not lead to overfitting [ISLR]. Thus we may plot the cumulative OOB error estimate for a fitted Random Forest and see where an increased number of trees give no error decrease. The OOB error estimate is the aggregated misclassification rate calculated on the training observations not in the bootstrap sample the tree was trained on.

For this initial fit to decide the amount of trees we use the standard value for $m$ in classification, $m=\sqrt{p}$. After choosing a numer of trees, we may compare the OOB error estimate for different values of $m$ and choose the model with lowest OOB error estimate. 

##### SVM



### Results

```{r}
set.seed(1337)

p <- ncol(longitudinal.train) - 1 # number of predictors
N <- nrow(longitudinal.train) # number of training observations


trainx <- select(longitudinal.train, -Demented)
trainy <- longitudinal.train$Demented


rf <- randomForest(Demented ~., data = longitudinal.train, ntree = 5000, mtry = sqrt(p), importance = TRUE)
err.df <- data.frame(x=1:5000, err = rf$err.rate[,1]) 

ggplot(err.df, aes(x=x)) + 
  geom_line(aes(y = err)) + 
  labs(x = "Iteration", y = "Out-Of-Bag error estimate") + 
  theme_minimal()
```

We see that 5000 trees is enough as the OOB error estimate stabilizes and it is not too slow to fit the model. Now we choose a value for $m$. We do this by computing the OOB error estimate for $m \in \{1,...,12\}$ as the amount of predictors is small enough so it is computationally feasible. 

```{r}
B = 5000
oob.df <- data.frame(m = 1:p, oob.err = rep(0,p))

for (m in 1:p){
  model <- randomForest(Demented ~., data = longitudinal.train,
                        mtry = m, ntree = B)
  oob.df$oob.err[m] <- model$err.rate[B,1] # save oob error
}

m <- oob.df$m[which.min(oob.df$oob.err)] # save best m value

ggplot(data = oob.df, aes(x = m)) + 
  geom_line(aes(y = oob.err)) + 
  geom_point(aes(y = oob.err), shape = 21, colour = "black", fill = "white", size = 2.8 ) +
  scale_x_continuous(breaks = seq(1,p,2)) +
  labs(x = "m", y = "OOB error")
```

Here we see that the best value for $m$ is $m = $ `oob.df$m[which.min(oob.df$oob.err)]`. Thus our best fit of a random forest uses $B = 5000$ trees and $m = 3$ randomly chosen predictors. 



### CV each method

```{r}
set.seed(1337)
#randomize data
longitudinal.train.rand <- longitudinal.train[sample(nrow(longitudinal.train)),]

#make 10 folds
folds <- cut(seq(1,nrow(longitudinal.train.rand)),breaks=10,labels=FALSE)

cv_err.df <- data.frame(fold = 1:10, logistic_err = rep(0,10),
                        rf_err = rep(0,10), svm_err = rep(1,10))

for(i in 1:10){
    #get test index
    test.index <- which(folds==i,arr.ind=TRUE)
    #separate test data and train data 
    test.data <- longitudinal.train.rand[test.index, ]
    train.data <- longitudinal.train.rand[-test.index, ]
    
    N <- length(test.data$Demented)
    rf <- randomForest(Demented ~. , data = train.data, mtry = 3, ntree = 5000) # fit random forest
    cv_err.df$rf_err[i] <- sum(predict(rf, newdata = test.data) != test.data$Demented) / N
    
    # logistic <- # fit logistic regression model 
    # cv_err.df$logistic_err[i] <- sum(predict(logistic, newdata = test.data) != test.data$Demented) / N
    
    # svm <- # fit svm 
    # cv_err.df$svm_err[i] <- sum(predict(svm, newdata = test.data) != test.data$Demented) / N
}

cv_err <- cv_err.df %>%
  select(-c("fold")) %>%
  colMeans()

```





<!--  Tinus' initial work here. -->



## Introduction: Scope and purpose of your project


## Descriptive data analysis/statistics


## Methods


## Results and interpretation


## Summary
