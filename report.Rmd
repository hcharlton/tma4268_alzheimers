---
title: "Compulsory Exercise 2: Prediction of Alzheimers Disease from Longitudinal MRI Data"
author:
- Chester Henry Charlton
- Kasper Eikeland
- Tinus Garshol
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: \usepackage{amsmath}
output:
  html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  #pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: "This is the place for your abstract (max 350 words)"
---
  
```{r setup, include=FALSE}
library("knitr")
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("rmarkdown")
library("dplyr")
library("GGally")
library("tidyr")
library("formatR")
```

```{r}
#setwd("/Users/chcharlton/NTNU/statLearning/compulsory_alzheimers")
setwd("/Users/kaspe/Documents/NTNU/23V/TMA4268_Statistical_Learning/Projects/tma4268_alzheimers")
```


<!--  Henry's initial work here -->


<!--  Kasper's initial work here. -->
```{r}
cross_sectional.df <- read.csv("oasis_cross-sectional.csv")

# Patient is demented if CDR is > 0
cs.mutated <- cross_sectional.df %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, CDR)) %>%
  na.omit() %>%
  mutate(Demented = as.factor(CDR>0),
         M.F = as.factor(M.F)) %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, Demented))


longitudinal.df <- read.csv("oasis_longitudinal.csv")

longitudinal_deltas <- longitudinal.df %>% 
  group_by(Subject.ID) %>%
  summarise(dMMSE = (last(MMSE) - first(MMSE)) / last(Visit), 
            deTIV = (last(eTIV) - first(eTIV)) / last(Visit),
            dnWBV = (last(nWBV) - first(nWBV)) / last(Visit),
            dASF = (last(ASF) - first(ASF)) / last(Visit),
            dAge = (last(Age) - first(Age)) / last(Visit))

baseline_with_deltas <- longitudinal.df %>%
  filter(Visit == 1) %>%
  left_join(longitudinal_deltas, by = c("Subject.ID")) %>%
  mutate(Demented = as.factor(Group != "Nondemented"), # Those who converted became demented
         M.F = as.factor(M.F)) %>% 
  dplyr::select(-c(Subject.ID, MRI.ID, Group, Visit, MR.Delay, Hand, CDR, SES)) 
  
set.seed(1337)

# Longitudinal data
l.train.indeces <- sample(1:nrow(baseline_with_deltas), floor(.75 * nrow(baseline_with_deltas)))
longitudinal.train <- na.omit(baseline_with_deltas[l.train.indeces,])
longitudinal.test <- na.omit(baseline_with_deltas[-l.train.indeces,])

# Cross-sectional
cs.train.indeces <- sample(1:nrow(cs.mutated), floor(.75 * nrow(cs.mutated)))
cs.train <- na.omit(cs.mutated[cs.train.indeces,])
cs.test <- na.omit(cs.mutated[-cs.train.indeces,])
```

```{r}
library("tree")
library("gbm")
library("randomForest")
```


```{r}
# Boosting, longitudinal
set.seed(1337)

# gbm wants response in {0,1}
longitudinal.train.boost <- mutate(longitudinal.train, Demented = ifelse(Demented == "FALSE", 0,1))
```



### Feature selection sensitivity test
Quick test to see if the feature selection is sensitive to the choice of parameters.
```{r}
# Feature selection sensitivity test
library("stepgbm")

trainx <- select(longitudinal.train.boost, -Demented)
trainy <- longitudinal.train.boost$Demented

# n.trees = 1000, 5000
# interaction.depth = 1, 2

feature.selection.boost <- stepgbm(trainx, trainy, family = "bernoulli",
                                   n.trees = 3000, learning.rate = 0.001,
                                   interaction.depth = 2, cv.fold = 10,
                                   rseed = 1337)


longitudinal.boost <- gbm(Demented ~., data = longitudinal.train.boost, 
                          distribution = "bernoulli", n.trees = 5000, 
                          interaction.depth = 2)

```


### Feature selection
We fix a set of parameters and find the best subset of features given these parameter values, chosen with AIC.

```{r}

```


### Paramter tuning
Now we tune the parameters to the chosen subset of predictors with K-fold cross validation. 

```{r}

```

This is all like a greedy algorithm for finding the best model out of all possible models by reducing the search space



Now that we have the best boosted model out of all the 
### CV each method

### Result
```{r}
longitudinal.test.boost <- mutate(longitudinal.test, Demented = ifelse(Demented == "FALSE", 0,1))

yhat.boost <- predict(longitudinal.boost, newdata = longitudinal.test.boost, 
                      n.trees = 5000, type = "response")
yhat.boost <- ifelse(yhat.boost > 0.5, 1,0)
table(pred = yhat.boost, truth = longitudinal.test.boost$Demented) # 0.8648649

```



<!--  Tinus' initial work here. -->



## Introduction: Scope and purpose of your project


## Descriptive data analysis/statistics


## Methods


## Results and interpretation


## Summary
