---
title: "Compulsory Exercise 2: Prediction of Alzheimers Disease from Longitudinal MRI Data"
author:
- Chester Henry Charlton
- Kasper Eikeland
- Tinus Garshol
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: \usepackage{amsmath}
output:
  html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  #pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: "This is the place for your abstract (max 350 words)"
---
  
```{r setup, include=FALSE}
library("knitr")
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE, # nolint
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("rmarkdown")
library("dplyr")
library("GGally")
library("tidyr")
library("formatR")
```

```{r}
#setwd("/Users/chcharlton/NTNU/statLearning/compulsory_alzheimers")
#setwd("/Users/kaspe/Documents/NTNU/23V/TMA4268_Statistical_Learning/Projects/tma4268_alzheimers")
setwd("C:/Users/tinus/OneDrive/Desktop/GitHub/tma4268_alzheimers")
```


```{r}
cross_sectional.df <- read.csv("oasis_cross-sectional.csv")
# Patient is demented if CDR is > 0
cs.mutated <- cross_sectional.df %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, CDR)) %>%
  na.omit() %>%
  mutate(Demented = as.factor(CDR>0),
         M.F = as.factor(M.F)) %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, Demented))
longitudinal.df <- read.csv("oasis_longitudinal.csv")
longitudinal_deltas <- longitudinal.df %>% 
  group_by(Subject.ID) %>%
  summarise(dMMSE = (last(MMSE) - first(MMSE)) / last(Visit), 
            deTIV = (last(eTIV) - first(eTIV)) / last(Visit),
            dnWBV = (last(nWBV) - first(nWBV)) / last(Visit),
            dASF = (last(ASF) - first(ASF)) / last(Visit),
            dAge = (last(Age) - first(Age)) / last(Visit))
baseline_with_deltas <- longitudinal.df %>%
  filter(Visit == 1) %>%
  left_join(longitudinal_deltas, by = c("Subject.ID")) %>%
  mutate(Demented = as.factor(Group != "Nondemented"), # Those who converted became demented
         M.F = as.factor(M.F)) %>% 
  dplyr::select(-c(Subject.ID, MRI.ID, Group, Visit, MR.Delay, Hand, CDR, SES)) 
  
set.seed(1337)
# Longitudinal data
l.train.indeces <- sample(1:nrow(baseline_with_deltas), floor(.75 * nrow(baseline_with_deltas)))
longitudinal.train <- na.omit(baseline_with_deltas[l.train.indeces,])
longitudinal.test <- na.omit(baseline_with_deltas[-l.train.indeces,])
# Cross-sectional
cs.train.indeces <- sample(1:nrow(cs.mutated), floor(.75 * nrow(cs.mutated)))
cs.train <- na.omit(cs.mutated[cs.train.indeces,])
cs.test <- na.omit(cs.mutated[-cs.train.indeces,])
```
```

```{r}
library("tree")
library("gbm")
library("randomForest")


set.seed(1337)
# Longitudinal data
train <- sample(1:nrow(baseline_with_deltas), floor(.75 * nrow(baseline_with_deltas)))
longitudinal.train <- na.omit(baseline_with_deltas[train,])
longitudinal.test <- na.omit(baseline_with_deltas[-train,])
```


```{r}
# Classification tree
set.seed(1337)
longitudinal.tree <- tree(Demented ~., longitudinal.train)
summary(longitudinal.tree)
plot(longitudinal.tree)
text(longitudinal.tree, pretty = 0)

# Test predictions
pred.tree <- predict(longitudinal.tree, newdata=longitudinal.test, type = "class")
table(pred = pred.tree, truth = longitudinal.test$Demented) # 0.7837838
# Pruning?
```


```{r}
# Bagging
set.seed(1337)
longitudinal.bag <- randomForest(Demented ~., longitudinal.train, mtry = 12, importance = TRUE)
summary(longitudinal.bag)
plot(longitudinal.bag)
yhat.bag <- predict(longitudinal.bag, newdata = longitudinal.test, type = "class")
table(pred = yhat.bag, truth = longitudinal.test$Demented) # 0.8648649
```


```{r}
# Random forest
set.seed(1337)
longitudinal.rf <- randomForest(Demented ~., longitudinal.train, importance = TRUE)
summary(longitudinal.rf)
plot(longitudinal.rf)
yhat.rf <- predict(longitudinal.rf, newdata = longitudinal.test, type = "class")
table(pred = yhat.rf, truth = longitudinal.test$Demented) # 0.8108108
```


```{r}
# Boosting
set.seed(1337)
longitudinal.train.boost <- mutate(longitudinal.train, Demented = ifelse(Demented == "FALSE", 0,1))
longitudinal.boost <- gbm(Demented ~., data = longitudinal.train.boost, 
                          distribution = "bernoulli", n.trees = 5000, interaction.depth = 2)
summary(longitudinal.boost)
longitudinal.test.boost <- mutate(longitudinal.test, Demented = ifelse(Demented == "FALSE", 0,1))
<<<<<<< Updated upstream
yhat.boost <- predict(longitudinal.boost, newdata = longitudinal.test.boost, n.trees = 5000, type = "response")
yhat.boost <- ifelse(yhat.boost > 0.5, 1,0)
table(pred = yhat.boost, truth = longitudinal.test.boost$Demented) # 0.8648649
=======
yhat.boost <- predict(longitudinal.boost, newdata = longitudinal.test.boost, n.trees = 1000, type = "response")
yhat.boost <- ifelse(yhat.boost > 0.5, 1,0) # nolint
table(yhat.boost, longitudinal.test.boost$Demented)
>>>>>>> Stashed changes
```


```{r}
# Logistic
longitudinal.logistic <- glm(Demented ~., family = binomial, data = longitudinal.train)
logistic.probs <- predict(longitudinal.logistic, newdata = longitudinal.test, type = "response")
yhat.logistic <- ifelse(logistic.probs > .5, 1,0)
table(pred = yhat.logistic, truth = longitudinal.test$Demented) # 0.8648649

```


# SVM
Attempt at data prediction using a SVM model. The "e1071" package has a built-in
svm function. It can also perform k-fold CV on the data (returns accuracy metric - perecentage of predictions it got right).
First step is to standardize data, usually yields better performance when fitting models.
(might be that the svm function does this internally)
```{r}
# Necessary packages
install.packages("e1071")
install.packages("caret")
library("e1071")
library("caret")

summary(longitudinal.train)
st.long.train <- longitudinal.train
# Standardize/normalize all the numeric features
st.long.train[,2:12] <- scale(st.long.train[,2:12]) 
summary(st.long.train)
```
Second step is to make a choice of kernel. Usually, as the feature space increases
data can tend to become more linearly seperable. So a linear kernel is a natural choice to test.
Polynomial kernels increase rapidly in complexity as dimensions increase. Also, adding
larger degree polynomial hyperplanes quickly result in overfitting of data. 
RBF-kernel, or a radial basis function kernel is most widely used in practice 
when using SVM's, so it would be interesting to compare with this one. 
Simgoid kernel is usually good for binary classification problems, but is avoided
in this investigation. In pre-testing, results did not seem promising for this kernel,
so it will be avoided here. Altough, it would be interesting to investigate it further
in a more detailed SVM analysis.
```{r}
set.seed(58008)
# Linear kernel
lin.svm <- svm(Demented~.,data=st.long.train,kernel="linear",cross=10)
summary(lin.svm)

# Radial kernel
rad.svm <-svm(Demented~.,data=st.long.train,kernel="radial",cross=10)
summary(rad.svm)
```
Both kernels perform decent, with accuracy > 75%. Linear kernel performs slightly better
under deafult choice of parameters. Now onto investigating tuning parameters.
Begin with tuning cost-parameter C. C "acts" as a weight of how hard we penalize
data that lies in the margin of the boundary in the model. 

C is the only tuning parameter for a linear kernel, so we start there.
```{r}
set.seed(58008)
# Testing accuracy for different costs "C" 
# We'll do multiple passes with k-fold CV using the training data. 
# Range of costs, by orders of magnitude (C=10^4 results in max iteration problems)

# Linear kernel
C = 10^(-4:3)
best_cost_lin = c()
for (j in 1:15) {
  accuracy = c()
  for (i in 1:length(C)) {
    lin.svm <- svm(Demented~.,data=st.long.train,kernel="linear",cost=C[i],cross=10,scale=FALSE)
    accuracy <- c(accuracy,lin.svm[29])
  }
  index <- which.max(accuracy)
  best_cost_lin <- c(best_cost_lin,C[index])
}

tab <- table(best_cost_lin)
y <- names(tab)
x <- as.numeric(tab)
barplot(x,names.arg = y,ylab="Frequenzy",xlab="Cost")
``` 
Cost of 1 seems to be most prevalent after 15 runs.

A SVM with radial kernel also has a tuning parameter gamma. Gamma is closely
related to the curvature of the seperating boundary, or the "stiffness" of it.
Need to test with both parameters simultaneously.
e1071 library has a built-in tune function to handle this.
Let's see how it performs.
```{r}
set.seed(58008)
# 10 runs with 10-fold CV. Selecting the best combination of gamma and C
# in each iteration. How to represent this data well?
for (i in 1:10) {
  obj <- tune(svm, Demented~., data = st.long.train,ranges = list(gamma = 10^(-4:3), cost = 10^(-4:3)),tunecontrol = tune.control(cross=10))
  GC <- summary(obj)[1]
  acc <- summary(obj)[2]
  acc <- (1-acc[[1]])*100
  best_cost <- GC[[1]]$cost
  best_gamma <- GC[[1]]$gamma
  k <- c(best_gamma,best_cost,acc)
  print(k)
}
```
It appears that a gamma of 0.01 and a cost of 100 produces the best results on average.
Now we have obtained a tuning of hyperparamters for both kernels.
Let's test the accuracy of both kernels with tuned hyperparameters.
```{r}
set.seed(58008)
# Linear kernel, cost = 1
lin.svm <- svm(Demented~.,data=st.long.train,kernel="linear",cost=1,cross=10)
lin_accuracy <- lin.svm[29]
print(lin_accuracy)
# Radial kernel, gamma = 0.01, cost = 100
radial.svm <- svm(Demented~.,data=st.long.train,kernel="radial",gamma=1e-02,cost=1e+02,cross=10)
rad_accuracy <- radial.svm[29]
print(rad_accuracy)
```
Both models have an accuracy of approx. 80%. Radial kernel under the given
parameters performs slightly better than linear. I wouldn't say the results are 
conclusive in determining which model is best.

This section is about how the SVM performs when only considering deltas/removing deltas.
It is outdated and may be omitted if it's unnecessary.
```{r}
set.seed(58008)
# Dataframes with only deltas (longitudinal data)
deltas.long <- baseline_with_deltas %>% na.omit() %>% select(c(dAge,dASF,deTIV,dMMSE,dnWBV,Demented)) 
deltas.index <- sample(1:nrow(deltas.long), floor(.75 * nrow(deltas.long)))
deltas.train <- deltas.long[deltas.index,]
deltas.test <- deltas.long[-deltas.index,]

deltas.svm <- svm(Demented~., data=deltas.train,kernel="linear",cost=10)
deltas.predict <- predict(deltas.svm, newdata=deltas.test,type="class")
table(deltas.predict, deltas.test$Demented)
# Result is horrible, no matter the kernel

set.seed(58008)
# Dataframes without deltas (longitudinal data)
nodelt.long <- baseline_with_deltas %>% select(c(Age,ASF,eTIV,MMSE,nWBV,Demented)) %>% na.omit()
nodelt.index <- sample(1:nrow(nodelt.long), floor(.75 * nrow(nodelt.long)))
nodelt.train <- nodelt.long[nodelt.index,]
nodelt.test <- nodelt.long[-nodelt.index,]

nodelt.svm <- svm(Demented~., data=nodelt.train,kernel="linear",cost=10)
nodelt.predict <- predict(nodelt.svm, newdata=nodelt.test,type="class")
table(nodelt.predict, nodelt.test$Demented)
# Result is still worse without deltas

confusionMatrix(nodelt.predict, nodelt.test$Demented)

```


```{r}
# LDA
library("MASS")

longitudinal.lda <- lda(Demented ~ ., data = longitudinal.train)
yhat.lda <- predict(longitudinal.lda, newdata = longitudinal.test)$class
table(pred = yhat.lda, truth = longitudinal.test$Demented) # 0.8108108

```


```{r}
# QDA
longitudinal.qda <- qda(Demented ~ ., data = longitudinal.train)
yhat.qda <- predict(longitudinal.qda, newdata = longitudinal.test)$class
table(pred = yhat.qda, truth = longitudinal.test$Demented) # 0.8648649
```



<!--  Tinus' initial work here. -->



## Introduction: Scope and purpose of your project


## Descriptive data analysis/statistics
```{r}

```

## Methods


## Results and interpretation


## Summary
