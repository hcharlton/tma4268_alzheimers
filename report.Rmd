---
title: 'Compulsory Exercise 2: Prediction of Alzheimers Disease from Longitudinal
  MRI Data'
author:
- Chester Henry Charlton
- Kasper Eikeland
- Tinus Garshol
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: no
    toc_depth: '2'
  pdf_document:
    toc: no
    toc_depth: '2'
header-includes: \usepackage{amsmath}
urlcolor: blue
abstract: "While Alzheimer's disease is traditionally diagnosed by medical professionals,
  in this report we detail the use of statistical learning models to classify Alzheimer's
  disease. Longitudinal MRI data was used from a mix of nondemented and demented patients.
  This data also included several non-MRI features, such as a short objective mental
  exam, education level, and the final diagnosis of dementia, which was used as a
  target variable for the models. The models examined were decision trees, support
  vector machines, and logistic regression. Support vector machines and decision trees
  were chosen based on applicability, and logistic regression was chosen due to its
  relevance and consistent use in the mdeical field. The selected models trained on
  a training subset of the data independently and then compared using cross-validation
  on the training data. From this the optimal model was selected, which finally was
  evaluated on the test subset of the data."
---

```{r setup, include=FALSE}
library("knitr")
knitr::opts_chunk$set(echo=TRUE, eval=TRUE, message=FALSE, warning = FALSE, 
                      strip.white = TRUE, prompt = FALSE, cache = TRUE, 
                      size = "scriptsize", fig.width = 6, fig.height = 4,
                      fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("randomForest")
library("rmarkdown")
library("dplyr")
library("GGally")
library("tidyr")
library("formatR")
library('DiagrammeR')
library('bestglm')
library("pROC") 
library("cvAUC")
library('webshot')
library('boot')
```

## The block below is specific to Henry's Computer
```{r}
#specific to Henry's computer
setwd("/Users/chcharlton/NTNU/statLearning/tma4268_alzheimers")
```



## Data Processing

```{r}
# Patient is demented if CDR is > 0
longitudinal.df <- read.csv("oasis_longitudinal.csv")
longitudinal_deltas <- longitudinal.df %>% 
  group_by(Subject.ID) %>%
  summarise(dMMSE = (last(MMSE) - first(MMSE)) / last(Visit), 
            deTIV = (last(eTIV) - first(eTIV)) / last(Visit),
            dnWBV = (last(nWBV) - first(nWBV)) / last(Visit),
            dASF = (last(ASF) - first(ASF)) / last(Visit),
            dAge = (last(Age) - first(Age)) / last(Visit)) %>% 
  na.omit()
baseline_with_deltas <- longitudinal.df %>%
  filter(Visit == 1) %>%
  left_join(longitudinal_deltas, by = c("Subject.ID")) %>%
  mutate(Demented = as.factor(Group != "Nondemented"), # Those who converted became demented
         M.F = as.factor(M.F)) %>% 
  dplyr::select(-c(Subject.ID, MRI.ID, Group, Visit, MR.Delay, Hand, CDR, SES))


set.seed(1337)
# Longitudinal data
l.train.indices <- sample(1:nrow(baseline_with_deltas), floor(.75 * nrow(baseline_with_deltas)))
longitudinal.train <- na.omit(baseline_with_deltas[l.train.indices,])
longitudinal.test <- na.omit(baseline_with_deltas[-l.train.indices,])
```

## EDA: 

```{r}
# select some quantitative data and plot the pairs of it
pairwise_scatter_data <- baseline_with_deltas[,c("Age", "EDUC", "MMSE",
                                   "eTIV", "nWBV", "ASF" )]
pairwise_scatter_data.deltas <-  baseline_with_deltas[,c("dMMSE",
                                   "deTIV", "dnWBV", "dASF")]

pair_plot <- ggpairs(data=pairwise_scatter_data)
pair_plot.deltas <- ggpairs(data=pairwise_scatter_data.deltas)
```
```{r}
# boxplots 
require(reshape2)

baseline_with_deltas.m <- melt(baseline_with_deltas[-1], id.var = "Demented")

boxplots <- 
  ggplot(data = baseline_with_deltas.m, aes(x=Demented, y=value)) + 
  geom_boxplot(aes(fill=Demented)) +
  theme_minimal()
boxplots.composite <- boxplots + facet_wrap( ~ variable, ncol=4, scales = 'free_y')


```



## select optimal features
```{r}
# use AIC to select features
longitudinal.best.aic <- bestglm(longitudinal.train, family = binomial, IC = "AIC", method ="forward")
logistic.best <- longitudinal.best.aic$BestModel
# get subset table as a dataframe for future use
longitudinal.aic.subsets <- 
  longitudinal.best.aic$Subsets %>% 
  data.frame() %>% 
  cbind(variables = 1:nrow(longitudinal.best.aic$Subsets))
# make feature-AIC plot
longitudinal.aic.subsets.plot <- 
  ggplot(longitudinal.aic.subsets, aes(x = variables, y = AIC)) +
  geom_point(color = "red", shape = "X", size = 4) +
  geom_line(color ='darkcyan') +
  ggtitle("Longitudinal AIC Subset Selection") +
  labs(x = "Variables", y = "AIC") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  theme_minimal() 
```






## CV for evaluating the performance of fitting the logistic model, using previously calculated AIC feature set

```{r}
set.seed(1337)
#randomize data
longitudinal.train.rand <- longitudinal.train[sample(nrow(longitudinal.train)),]

#make n folds
n <- 5
folds <- cut(seq(1,nrow(longitudinal.train.rand)),breaks=n,labels=FALSE)

#vector to store sensititivities
sens.vec <- c()
#specificities
spec.vec <- c()
#misclassivication errors
misc.vec <- c()
# cumulative confusion matrix
logistic.confusion.cumulative <- matrix(data = c(0,0,0,0), nrow=2,ncol=2)


for(i in 1:n){
    #get test index
    test.index <- which(folds==i,arr.ind=TRUE)
    #separate test data and train data 
    test.data <- longitudinal.train.rand[test.index, ]
    train.data <- longitudinal.train.rand[-test.index, ]
    #train best logistic model for training partition
    logistic.i <- glm(Demented ~ Age + MMSE + nWBV + dMMSE, data = train.data, family = binomial)
    #generated predicted probabilities on test partition 
    logistic.i.probs <- predict(logistic.i, newdata = test.data, type = "response")
    #convert probabilities to classification based on a P = 0.5 decision threshold
    logistic.i.pred <- ifelse(logistic.i.probs > 0.5, 1, 0)
    # generate confusion table for predictions
    logistic.i.confusion <- table(logistic.i.pred, test.data$Demented)
    rownames(logistic.i.confusion) <- c('pred 0','pred 1')
    colnames(logistic.i.confusion) <- c('true 0','true 1')
    # calculate sensitivity and specificity
    logistic.sens <- logistic.i.confusion[2,2] / sum(logistic.i.confusion[,2])
    logistic.spec <- logistic.i.confusion[1,1] / sum(logistic.i.confusion[,1])
    logistic.misc <- (logistic.i.confusion[2,1] + logistic.i.confusion[1,2]) / sum(logistic.i.confusion)
    #append accuracy metrics to persistent vectors
    sens.vec <- c(sens.vec,logistic.sens)
    spec.vec <- c(spec.vec,logistic.spec)
    misc.vec <- c(misc.vec, logistic.misc)
    #add confusion matrix to persistent object
    logistic.confusion.cumulative <- logistic.i.confusion + logistic.confusion.cumulative
}
# compute cumulative sensitivity, specificity and misclassification error accross the folds
sens.cumulative <- logistic.confusion.cumulative[2,2] / sum(logistic.confusion.cumulative[,2])
spec.cumulative <- logistic.confusion.cumulative[1,1] / sum(logistic.confusion.cumulative[,1])
misc.cumulative <- (logistic.confusion.cumulative[2,1] + logistic.confusion.cumulative[1,2]) /
  sum(logistic.confusion.cumulative)


sens_spec_misc <- c(sens.cumulative, spec.cumulative, misc.cumulative)
sens.vec
spec.vec
```

```{r}
# visualize some things
sens_spec_misc.df <- data.frame(sens = sens.vec, spec = spec.vec, misc = misc.vec)

logistic.accuracy.plot <- 
  ggplot(sens_spec_misc.df, aes(x = spec, y = sens))+
  geom_point(aes(size = misc)) + 
  labs(size = "Misclassification") +
  scale_x_reverse() +
  ggtitle("K-Fold Accuracy Distribution")+
  labs(x = "Specificity", y = "Sensitivity") +
  theme_minimal() 


logistic.accuracy.plot
logistic.confusion.cumulative
sens_spec_misc
```


## Train final model
```{r}
# train logistic model on entire training set
longitudinal.logistic <- glm(Demented ~ Age + MMSE + nWBV + dMMSE, data = longitudinal.train, family = binomial)
# extract training probabilities
longitudinal.logistic.train.probs <- longitudinal.logistic$fitted.values
# extract training classifications
longitudinal.logistic.train.preds <- ifelse(longitudinal.logistic.train.probs > 0.5, 1, 0)
# generate confusion table for model on training data
longitudinal.logistic.confusion <- table(longitudinal.logistic.train.preds, longitudinal.train$Demented)
rownames(longitudinal.logistic.confusion) <- c('pred 0','pred 1')
colnames(longitudinal.logistic.confusion) <- c('true 0','true 1')
# sensitivity and specificity
longitudinal.logistic.sens <- longitudinal.logistic.confusion[2,2] / sum(longitudinal.logistic.confusion[,2])
longitudinal.logistic.spec <- longitudinal.logistic.confusion[1,1] / sum(longitudinal.logistic.confusion[,1])
# misclassification error
longitudinal.logistic.misc <- (longitudinal.logistic.confusion[2,1] + longitudinal.logistic.confusion[1,2]) /
  sum(longitudinal.logistic.confusion)
#accuracy vector
sens_spec_misc <- c(longitudinal.logistic.sens, longitudinal.logistic.spec, longitudinal.logistic.misc)
sens_spec_misc
```



## plotting the master-diagram

```{r}
masterplan <- 
mermaid("
graph TD
    start[source data]
    process[processed data]
    start --> process[processed data]
    process --> a0
    process --> a1


    a0 -->  b0 
    a0 -->  b1 
    a0 -->  b2
    a0[training data]
    a1[test data]
    b0[SVM]
    b1[Boosting]
    b2[Logistic]
    c0[subsetted SVM]
    c1[subsetted Boosting]
    
    d0[best SVM]
    d1[best Boosting]
    d2[best Logistic]
    
    e0[choose best of best models with CV]
    
    b0 --> |feat param sensitivity| c0
    b1 --> |feat param sensitivity| c1
    b2 --> |feature selection| d2
    
    c0 --> |parameter tuning| d0
    c1 --> |parameter tuning| d1
    



    a1 --> best_test
    d0 --> e0
    d1 --> e0
    d2 --> e0
    e0 --> best_test
    best_test[evaluate finaln model on test data]
    best_test --> final[interpretation]
    

    
")
```



## Introduction: Scope and purpose of your project

The goal of this project is to develop a model that can classify the diagnosis of dementia, specifically Alzheimer's. To do this we use quantified data from MRI scans and objective psychological tests to form the basis of prediction. Data was found from Kaggle, however the original data was collated by the Open Access Series of Imaging Studies. The dataset includes MRI visits of 150 individuals between the ages of 60 and 96, providing data for several different visits of each patient to an MRI center, at which time a short psychological exam was also taken. In addition to this, the data includes a clinical diagnosis of dementia (either demented or nondemented) for the patient which we use as a target variable. While a simple and interpretable model is desirable, our primary goal is to develop a model that can rule out some patients from further treatment, as this would reduce the load on medical professionals, particularly doctors. As such, our main focus is prediction rather than inference since we are mainly concerned with our ability to emulate the assessment of doctors on these patients. Withdstanding this, we chose a small but diverse set of models: Random forest decision trees, support vector machines, and logistic regression. Within these, logistic regression is by far the most interpretable model, and also is relevant if only for its ongoing use in the medical field. 


## Descriptive data analysis/statistics


```{r}
pair_plot

pair_plot.deltas

boxplots.composite
```




### Description of Features:

MRI.ID: The unique identifier of an MRI scan. This feature was not used. 

Group: The group that the patient was classified in the across the various scans. Possible values are "Undemented," denoting a patient without symptoms of dementia throughout all scans, "Demented" denoting a patient with symptoms of dementia across all scans, or "Converted," denoting someone who began without symptoms of dementia but developed them at a further scan. We chose to throw away "Converted," since our primary goal is to develop a classification model for patients who either have dementia or not, and the Converted class complicates this since patients in this class would fall into different categories at different times. 

Visit: The order of the visit in which the scan was done, indexed from 1. This data was not used for our model, but was used for processing the data. 

MR Delay: The number of days since the previous scan. Marked as 0 for the initial scan. Similarly, this data was not used for the model directly, but helped us for data processing as will be described later. 

M.F: Categorical variable for male or female patients at birth. M denotes male, F denotes female. This binary variable was included in the models, however feature selection points to it not being a decisive feature for classification. 

Hand: The patient's dominant hand. All of the patients were right-handed. This feature was thrown out. 

EDUC: The education level of the patient, measured in years (including elementary school). 

SES: "Socioeconomic status as assessed by the Hollingshead Index of Social Position and classified into categories from 1 (highest status) to 5 (lowest status) (Hollingshead, 1957)" 
The socioeconomic status of the patients as classified by the Hollingshead Index. Values range from 1 (most elite status) to 5 (lowest status). This feature was thrown out since it as not consistently recorded accross the patients.  

MMSE: The score of the Mini Mental State Examination. Ranges from worst (0) to best (30), and represents a measure of cognitive impairment. An example of this test is attached as an appendix. We determined from this example the that score is objective, and as such is relevant to use for an analysis where the goal is to reduce pressure on medical professionals. Due to the objective nature of the exam, it could theoretically be performed by an automated system. 

CDR: The patient's clinical dementia rating, as assessed by a medical professional. Ranges from 0 to 3, with any score above 0 representing a form of dementia. This is our target feature for classification, which we transformed into a binary variable. 

ASF: The atlas scaling factor of the patient. A measure standardizing the intra-cranial volume. 

eTIV: Estimated total intracranial volume. 

nWBV: Normalized whole-brain volume. This feature is measured as the percent of the possible total volume that is detected as grey or white matter. 


## Methods


### Data processing
The source data that we accessed had a number of peculiarities that needed to be resolved before we could use it. First and foremost, we noted that there were some features that were not feasible to use. These included Subject.ID, MRI.ID, Hand, and SES. Subject.ID is not useful to us as it is simply a unique identifier of the patient. This might have been useful if we were integrating this data into a larger context of data, but here it is not applicable. MRI.ID is similarly not useful since it is a unique identifier of the particular MRI scan. Following guidelines for tidy data, we are keeping each row as a unique observation (wide format) and so this is not necessary. Hand is a feature stating the dominant hand of the patient, and since all of the patients in this study were right-handed, it is not a valid feature to use as a predictor. SES denotes the socioeconomic status of the patient. This would have been an interesting feature to examine for predictive capacity, but it was not exhaustively collected like the other features and so many of the observations had this feature missing. Our two options given this were either to (1) throw out SES as a feature, or (2) remove observations that had this feature missing. Since such a large number of the observations were missing SES, we determined that it would be better to sacrifice it as a feature and preserve the size of our observation set. 

Beyond removing features, the next peculiarity is the longitudinal nature of the dataset, meaning that it includes multiple observations of the same individual at different points in time. The methods that we have learned in this course do not pertain to time-series data, so our solution was to add features, where relevant, that capture the basic properties of the time series information. MMSE, eTIV, nWBV, and ASF were use to create delta features (denoted as dMMSE, deTIV, etc) that represent the difference between the first and last observations of a particular feature for a particular patient, standardized with respect for time. For example, if a particular patient has their first MMSE recording stated as 28 and last as 22 with 300 days in between the measurements, their dMMSE is $\frac{28-22}{300} = 0.02$. This allows us to use the information conveyed by the longitudinal data while still using the methods from this course. 



### Model Descriptions

Logistic regression:
This method was chosen as a candidate due to its continued use in the medical field and value as a method to give classification probabilities. This is especially useful in our case since we have a binary target variable (demented or nondemented) and a combination of binary and nonbinary predictors. Logistic regression fits a function for $n$ variables that outputs a probability between $0$ and $1$ for each observation. It is in this sense that logistic regression is useful for classification, since with that set of probababilities for each observation, we can implement a decision threshold in order to make the prediction bivalent. For instance, we could choose a threshold of 0.6, and the process would go like this: Our logistic model outputs a probability of 0.4 that a particular observation belongs to the target class, and then that observation gets a classification of $0$, denoting that it does not belong to the target class. The logistic model outputs a probability of 0.62 that a particular observation belongs to the target class, and then that observation gets a classification of $1$, denoting that it belongs to the target class. By tuning this threshold we can achieve different characteristics for our model, such as different sensitivities and specificities for the classification. As we lower the threshold for the classification, the sensitivity increases and specificity decreased, and similarly when we raise the threshold, the sensitivity decreases and the specificity increases. As an example, imagine that we decreased the threshold entirely to its lowest possible value, $0$. This would mean that every observation would be classified as within the target class, and it is trivially obvious that $P(A|B = 1.0)$, where $A$ stands for "the observation is classified into the target class," and $B$ stands for "the observation truly belongs to the target class.

[how does it work] 
[strengths] 
[weaknesses]
[applicability for this project]
[potential limitations]
[how are we tuning this model]
Subset selection: we choose to use the Aikake Information Criterion. The possibilities we could have chosen for this were Cp, BIC, AIC, and Adjusted R^2. Cp and Adjusted R^2 are not appropriate since they rely on RSS, which is not compatible with a logistic function, and after this we arbitrarily chose AIC instead of BIC, which based on our knowledge will not be significant. This 

Boosting decision trees:
[how does it work] 
[strengths] 
[weaknesses]
[applicability for this project]
[potential limitations]
[how are we tuning this model]

Support vector machines:
[how does it work] 
[strengths] 
[weaknesses]
[applicability for this project]
[potential limitations]
[how are we tuning this model]


### Model Selection 

Here we will provide an overview of the process we followed to arrive at a final model. 

1. Collect the source data. 

2. Process data: select relevant features, add deltas, remove time series data.  

3. Separate processed data into training and test partitions.

3. Choose a small set of hyperparameters randomly for selected models, test feature selection sensitivity to this test.

4. Once feature selection is demonstrated to be insensitive to hyperparameters, proceed with feature selection and
arbitrary hyper parameters.

5. Tune hyperparameters (as applicable)

6. Compare SVM, Boosting, and Logistic models by simulatentously testing with k-folded training data. 

7. Select best model from the three.

8. Evaluate best model on test partition. 


```{r}
masterplan
```
## Model evaluation 
[what's our loss function]
[how do we propose to compare the best models, once we have them]


## Results and interpretation
comparison items: 
[model size] 
[flexibility] 
[bias-variance trade-off]


### points to discuss:

whether to include MMSE: Seems like it is objective, and only takes a few minutes to run. It could theoretically be run by existing NLP structures, so we could make an argument that using this does not compromise our goal of eliminating pressure on medical professionals. 

whether having an high specificity or sensitivity is more important: what is worse: (1) someone who truly does not have alzheimer's being told they do, or (2) someone who truly does have alzheimer's being told they don't? If (1): 


overall strategy like Kasper drew:
treat the entire process as model selection. separate test and training data. Set aside test data until we choose a final best model based on the training data.  train models on training data, setting an arbitrary set of parameters -> perform a rough sensitivity analysis for the effect of parameters on the feature selection -> choose arbitrary parameters -> perform feature selection, choosing best set of features -> tune parameters for the model with best feature set -> evaluate all three models with k-fold cv in parallel -> select best model -> 

why choose first visit as baseline instead of some other one or an average?
why only consider the change between the first and last? ]
consider adding a feature for fluctuation in other features (ie how much does eTIV fluctuate, regardless of direction)


## Summary












