---
title: "Compulsory Exercise 2: Prediction of Alzheimers Disease from Longitudinal MRI Data"
author:
- Chester Henry Charlton
- Kasper Eikeland
- Tinus Garshol
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: \usepackage{amsmath}
output:
  html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  #pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: "This is the place for your abstract (max 350 words)"
---
  
```{r setup, include=FALSE}
library("knitr")
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE, # nolint
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("rmarkdown")
library("dplyr")
library("GGally")
library("tidyr")
library("formatR")
```

```{r}
#setwd("/Users/chcharlton/NTNU/statLearning/compulsory_alzheimers")
#setwd("/Users/kaspe/Documents/NTNU/23V/TMA4268_Statistical_Learning/Projects/tma4268_alzheimers")
setwd("C:/Users/tinus/OneDrive/Desktop/GitHub/tma4268_alzheimers")
```


```{r}
cross_sectional.df <- read.csv("oasis_cross-sectional.csv")
# Patient is demented if CDR is > 0
cs.mutated <- cross_sectional.df %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, CDR)) %>%
  na.omit() %>%
  mutate(Demented = as.factor(CDR>0),
         M.F = as.factor(M.F)) %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, Demented))
longitudinal.df <- read.csv("oasis_longitudinal.csv")
longitudinal_deltas <- longitudinal.df %>% 
  group_by(Subject.ID) %>%
  summarise(dMMSE = (last(MMSE) - first(MMSE)) / last(Visit), 
            deTIV = (last(eTIV) - first(eTIV)) / last(Visit),
            dnWBV = (last(nWBV) - first(nWBV)) / last(Visit),
            dASF = (last(ASF) - first(ASF)) / last(Visit),
            dAge = (last(Age) - first(Age)) / last(Visit))
baseline_with_deltas <- longitudinal.df %>%
  filter(Visit == 1) %>%
  left_join(longitudinal_deltas, by = c("Subject.ID")) %>%
  mutate(Demented = as.factor(Group != "Nondemented"), # Those who converted became demented
         M.F = as.factor(M.F)) %>% 
  dplyr::select(-c(Subject.ID, MRI.ID, Group, Visit, MR.Delay, Hand, CDR, SES)) 
  
set.seed(1337)
# Longitudinal data
l.train.indeces <- sample(1:nrow(baseline_with_deltas), floor(.75 * nrow(baseline_with_deltas)))
longitudinal.train <- na.omit(baseline_with_deltas[l.train.indeces,])
longitudinal.test <- na.omit(baseline_with_deltas[-l.train.indeces,])
# Cross-sectional
cs.train.indeces <- sample(1:nrow(cs.mutated), floor(.75 * nrow(cs.mutated)))
cs.train <- na.omit(cs.mutated[cs.train.indeces,])
cs.test <- na.omit(cs.mutated[-cs.train.indeces,])
```
```

```{r}
library("tree")
library("gbm")
library("randomForest")


set.seed(1337)
# Longitudinal data
train <- sample(1:nrow(baseline_with_deltas), floor(.75 * nrow(baseline_with_deltas)))
longitudinal.train <- na.omit(baseline_with_deltas[train,])
longitudinal.test <- na.omit(baseline_with_deltas[-train,])
```


```{r}
# Classification tree
set.seed(1337)
longitudinal.tree <- tree(Demented ~., longitudinal.train)
summary(longitudinal.tree)
plot(longitudinal.tree)
text(longitudinal.tree, pretty = 0)

# Test predictions
pred.tree <- predict(longitudinal.tree, newdata=longitudinal.test, type = "class")
table(pred = pred.tree, truth = longitudinal.test$Demented) # 0.7837838
# Pruning?
```


```{r}
# Bagging
set.seed(1337)
longitudinal.bag <- randomForest(Demented ~., longitudinal.train, mtry = 12, importance = TRUE)
summary(longitudinal.bag)
plot(longitudinal.bag)
yhat.bag <- predict(longitudinal.bag, newdata = longitudinal.test, type = "class")
table(pred = yhat.bag, truth = longitudinal.test$Demented) # 0.8648649
```


```{r}
# Random forest
set.seed(1337)
longitudinal.rf <- randomForest(Demented ~., longitudinal.train, importance = TRUE)
summary(longitudinal.rf)
plot(longitudinal.rf)
yhat.rf <- predict(longitudinal.rf, newdata = longitudinal.test, type = "class")
table(pred = yhat.rf, truth = longitudinal.test$Demented) # 0.8108108
```


```{r}
# Boosting
set.seed(1337)
longitudinal.train.boost <- mutate(longitudinal.train, Demented = ifelse(Demented == "FALSE", 0,1))
longitudinal.boost <- gbm(Demented ~., data = longitudinal.train.boost, 
                          distribution = "bernoulli", n.trees = 5000, interaction.depth = 2)
summary(longitudinal.boost)
longitudinal.test.boost <- mutate(longitudinal.test, Demented = ifelse(Demented == "FALSE", 0,1))
<<<<<<< Updated upstream
yhat.boost <- predict(longitudinal.boost, newdata = longitudinal.test.boost, n.trees = 5000, type = "response")
yhat.boost <- ifelse(yhat.boost > 0.5, 1,0)
table(pred = yhat.boost, truth = longitudinal.test.boost$Demented) # 0.8648649
=======
yhat.boost <- predict(longitudinal.boost, newdata = longitudinal.test.boost, n.trees = 1000, type = "response")
yhat.boost <- ifelse(yhat.boost > 0.5, 1,0) # nolint
table(yhat.boost, longitudinal.test.boost$Demented)
>>>>>>> Stashed changes
```


```{r}
# Logistic
longitudinal.logistic <- glm(Demented ~., family = binomial, data = longitudinal.train)
logistic.probs <- predict(longitudinal.logistic, newdata = longitudinal.test, type = "response")
yhat.logistic <- ifelse(logistic.probs > .5, 1,0)
table(pred = yhat.logistic, truth = longitudinal.test$Demented) # 0.8648649

```


# SVM
Attempt at data prediction using a SVM model. The "e1071" package has a built-in
svm function. It can also perform k-fold CV on the data (returns accuracy metric - perecentage of predictions it got right).
First step is to standardize data, usually yields better performance when fitting models.
(might be that the svm function does this internally)
```{r}
# Necessary packages
install.packages("e1071")
install.packages("caret")
library("e1071")
library("caret")

summary(longitudinal.train)
st.long.train <- longitudinal.train
# Standardize/normalize all the numeric features
st.long.train[,2:12] <- scale(st.long.train[,2:12]) 
summary(st.long.train)
```
Second step is to make a choice of kernel. Usually, as the feature space increases
data can tend to become more linearly seperable. So a linear kernel is a natural choice to test.
Polynomial kernels increase rapidly in complexity as dimensions increase. Also, adding
larger degree polynomial hyperplanes quickly result in overfitting of data. 
Possibly radial kernel as the second choice of kernel.
```{r}
set.seed(58008)
# Linear kernel
lin.svm <- svm(Demented~.,data=st.long.train,kernel="linear",cross=10)
summary(lin.svm)

# Radial kernel
rad.svm <-svm(Demented~.,data=st.long.train,kernel="radial",cross=10)
summary(rad.svm)
```
Both kernels perform decent, with accuracy > 75%. Linear kernel performs slightly better
under deafult choice of parameters. Now onto investigating tuning parameters.
Begin with tuning cost-parameter C. C "acts" as a weight of how hard we penalize
data that lies in the margin of the boundary in the model.  
```{r}
set.seed(58008)
# Testing for different costs "C" 
# We'll do multiple passes with k-fold CV using the training data. 
# Then choose the average C
# Range of costs, by orders of magnitude

# Linear kernel
C = 10^(-4:3)
best_cost_lin = c()
for (j in 1:10) {
  accuracy = c()
  for (i in 1:length(C)) {
    lin.svm <- svm(Demented~.,data=st.long.train,kernel="linear",cost=C[i],cross=10,scale=FALSE)
    accuracy <- c(accuracy,lin.svm[29])
  }
  index <- which.max(accuracy)
  best_cost_lin <- c(best_cost_lin,C[index])
}
print("Linear C:")
print(best_cost_lin)
``` 
Either a cost of 1 or 10 seems to be producing the best results. Let's test the accuracy of both
```{r}
set.seed(58008)
# Cost = 1
lin.svm <- svm(Demented~.,data=st.long.train,kernel="linear",cost=1,cross=10)
accuracy_1 <- lin.svm[29]
print(accuracy_1)

# Cost = 10
lin.svm <- svm(Demented~.,data=st.long.train,kernel="linear",cost=10,cross=10)
accuracy_10 <- lin.svm[29]
print(accuracy_10)
```

A SVM with radial kernel also has a tuning parameter gamma. Need to test with both 
parameters simultaneously. e1071 library has a built-in tune function. 
Let's see how it performs.
```{r}
set.seed(58008)
for (i in 1:10) {
  obj <- tune(svm, Demented~., data = st.long.train,ranges = list(gamma = 10^(-4:3), cost = 10^(-4:3)),tunecontrol = tune.control(cross=10))
  GC <- summary(obj)[1]
  best_cost <- GC[[1]]$cost
  best_gamma <- GC[[1]]$gamma
  k <- c(best_gamma,best_cost)
  print(k)
}
```
Most dynamic duo: gamma = 1e-02, cost = 1e+02. Let's test it's accuracy
```{r}
set.seed(58008)
radial.svm <- svm(Demented~.,data=st.long.train,kernel="radial",gamma=1e-02,cost=1e+02,cross=10)
accuracy <- radial.svm[29]
print(accuracy)
```
Accuracy yield is higher than before, result seems promising. 
```{r}
set.seed(58008)
# Dataframes with only deltas (longitudinal data)
deltas.long <- baseline_with_deltas %>% na.omit() %>% select(c(dAge,dASF,deTIV,dMMSE,dnWBV,Demented)) 
deltas.index <- sample(1:nrow(deltas.long), floor(.75 * nrow(deltas.long)))
deltas.train <- deltas.long[deltas.index,]
deltas.test <- deltas.long[-deltas.index,]

deltas.svm <- svm(Demented~., data=deltas.train,kernel="linear",cost=10)
deltas.predict <- predict(deltas.svm, newdata=deltas.test,type="class")
table(deltas.predict, deltas.test$Demented)
# Result is horrible, no matter the kernel

set.seed(58008)
# Dataframes without deltas (longitudinal data)
nodelt.long <- baseline_with_deltas %>% select(c(Age,ASF,eTIV,MMSE,nWBV,Demented)) %>% na.omit()
nodelt.index <- sample(1:nrow(nodelt.long), floor(.75 * nrow(nodelt.long)))
nodelt.train <- nodelt.long[nodelt.index,]
nodelt.test <- nodelt.long[-nodelt.index,]

nodelt.svm <- svm(Demented~., data=nodelt.train,kernel="linear",cost=10)
nodelt.predict <- predict(nodelt.svm, newdata=nodelt.test,type="class")
table(nodelt.predict, nodelt.test$Demented)
# Result is still worse without deltas

confusionMatrix(nodelt.predict, nodelt.test$Demented)

set.seed(58008)
SVM <- svm(Demented~.,data=longitudinal.train,kernel="linear",cross=10)
print(SVM[29])

c = "Age"
formula <- paste("Demented~", paste(c, collapse=" + "))
svm(formula,data=longitudinal.train)
```


```{r}
# LDA
library("MASS")

longitudinal.lda <- lda(Demented ~ ., data = longitudinal.train)
yhat.lda <- predict(longitudinal.lda, newdata = longitudinal.test)$class
table(pred = yhat.lda, truth = longitudinal.test$Demented) # 0.8108108

```


```{r}
# QDA
longitudinal.qda <- qda(Demented ~ ., data = longitudinal.train)
yhat.qda <- predict(longitudinal.qda, newdata = longitudinal.test)$class
table(pred = yhat.qda, truth = longitudinal.test$Demented) # 0.8648649
```



<!--  Tinus' initial work here. -->



## Introduction: Scope and purpose of your project


## Descriptive data analysis/statistics
```{r}

```

## Methods


## Results and interpretation


## Summary
