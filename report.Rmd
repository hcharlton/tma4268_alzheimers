---
title: "Compulsory Exercise 2: Prediction of Alzheimers Disease from Longitudinal MRI Data"
author:
- Chester Henry Charlton
- Kasper Eikeland
- Tinus Garshol
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: \usepackage{amsmath}
output:
  html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  #pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: "This is the place for your abstract (max 350 words)"
---
  
```{r setup, include=FALSE}
library("knitr")
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("rmarkdown")
library("dplyr")
library("GGally")
library("tidyr")
library("formatR")
```

```{r}
#setwd("/Users/chcharlton/NTNU/statLearning/compulsory_alzheimers")
setwd("/Users/kaspe/Documents/NTNU/23V/TMA4268_Statistical_Learning/Projects/tma4268_alzheimers")
```


<!--  Henry's initial work here -->


<!--  Kasper's initial work here. -->
```{r}
cross_sectional.df <- read.csv("oasis_cross-sectional.csv")

# Patient is demented if CDR is > 0
cs.mutated <- cross_sectional.df %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, CDR)) %>%
  na.omit() %>%
  mutate(Demented = as.factor(CDR>0),
         M.F = as.factor(M.F)) %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, Demented))


longitudinal.df <- read.csv("oasis_longitudinal.csv")

longitudinal_deltas <- longitudinal.df %>% 
  group_by(Subject.ID) %>%
  summarise(dMMSE = (last(MMSE) - first(MMSE)) / last(Visit), 
            deTIV = (last(eTIV) - first(eTIV)) / last(Visit),
            dnWBV = (last(nWBV) - first(nWBV)) / last(Visit),
            dASF = (last(ASF) - first(ASF)) / last(Visit),
            dAge = (last(Age) - first(Age)) / last(Visit))

baseline_with_deltas <- longitudinal.df %>%
  filter(Visit == 1) %>%
  left_join(longitudinal_deltas, by = c("Subject.ID")) %>%
  mutate(Demented = as.factor(Group != "Nondemented"), # Those who converted became demented
         M.F = as.factor(M.F)) %>% 
  dplyr::select(-c(Subject.ID, MRI.ID, Group, Visit, MR.Delay, Hand, CDR, SES)) 
  
set.seed(1337)

# Longitudinal data
l.train.indeces <- sample(1:nrow(baseline_with_deltas), floor(.75 * nrow(baseline_with_deltas)))
longitudinal.train <- na.omit(baseline_with_deltas[l.train.indeces,])
longitudinal.test <- na.omit(baseline_with_deltas[-l.train.indeces,])

# Cross-sectional
cs.train.indeces <- sample(1:nrow(cs.mutated), floor(.75 * nrow(cs.mutated)))
cs.train <- na.omit(cs.mutated[cs.train.indeces,])
cs.test <- na.omit(cs.mutated[-cs.train.indeces,])
```

```{r}
library("tree")
library("gbm")
library("randomForest")
```


```{r}
# Boosting, longitudinal
set.seed(1337)

# gbm wants response in {0,1}
longitudinal.train.boost <- mutate(longitudinal.train, Demented = ifelse(Demented == "FALSE", 0,1))
```



### Feature selection sensitivity test
Quick test to see if the feature selection is sensitive to the choice of parameters.
```{r}
# Feature selection sensitivity test
library("stepgbm")

trainx <- select(longitudinal.train.boost, -Demented)
trainy <- longitudinal.train.boost$Demented

# n.trees = 1000, 5000
# interaction.depth = 1, 2

feature.selection.boost <- stepgbm(trainx, trainy, 
                                   family = "bernoulli",
                                   n.trees = 3000, 
                                   learning.rate = 0.1,
                                   interaction.depth = 2, 
                                   cv.fold = 10,
                                   rseed = 1337, 
                                   keep.data = TRUE)

feature.selection.boost <- stepgbm(trainx, trainy, 
                                   method = "KIRVI", 
                                   family = "bernoulli", 
                                   rpt = 2,
                                   predacc = "VEcv",
                                   cv.fold = 5,
                                   min.n.var = 2,
                                   n.cores = 2,
                                   delta.predacc = 0.01,
                                   rseed = 1337)

feature.selection.boost <- stepgbmRVI(trainx, trainy, 
                                      family = "bernoulli", 
                                      n.trees = 100, 
                                      interaction.depth = 2, 
                                      learning.rate = 0.01, 
                                      cv.fold = 5, 
                                      n.cores = 2, 
                                      min.n.var = 1, 
                                      rseed = 1234)

longitudinal.boost <- gbm(Demented ~., data = longitudinal.train.boost, 
                          distribution = "bernoulli", n.trees = 2000, 
                          interaction.depth = 2)

# backwards feature selection

backwards.gbm <- function(trainx, trainy, B, d, shrinkage) {
  # Step 1
  M_p <- gbm(trainy ~ ., data = trainx, distribution = "bernoulli", n.trees = B, interaction.depth = d, shrinkage = shrinkage) # full model
  
  # Step 2
  M_k_list <- data.frame(predictors = rep(0, ncol(trainx)), misclass_rate = rep(0, ncol(trainx)))
  
  predictors_k <- c(1:ncol(trainx)) # start with all predictors
  for (k in ncol(trainx):2) { # iterate backwards
    # Consider all k models that contain all but one of the predictors in M_k for a total of k-1 predictors

    results_k.df <- data.frame(predictors = I(rep(list(rep(0,k-1)), k)), 
                               misclass_rate = rep(0, k)) # k rows, 2 columns, first column is which predictors
    
    for (j in 1:k) { # iterate through predictors_k
      current_predictors <- predictors_k[-j]  # exclude jth predictor
      
      M_k_tmp <- gbm(trainy ~ ., data = select(trainx, current_predictors),
                     distribution = "bernoulli", n.trees = B, interaction.depth = d, 
                     shrinkage = shrinkage) # fit model with current selection of predictors
      
      # save training loss and corresponding predictors in a list or something
      results_k.df$predictors[j] <- list(current_predictors)
      results_k.df$misclass_rate[j] <- sum(as.numeric(predict(M_k_tmp, type = "response") > 0.5)  != trainy) / length(trainy) # calculate misclassification rate, 0.5 cutoff

    }
    
    best.model_k <- which.min(results_k.df$misclass_rate)# select model with minimum training loss
    
    predictors_k <- unlist(results_k.df$predictors[best.model_k]) # get predictors from model M_k, preferably vector of indeces
    
    M_k_list$predictors[k] <- list(predictors_k) # save which predictors give min misclass rate for k predictors
    M_k_list$misclass_rate[k] <- results_k.df$misclass_rate[best.model_k]
  }
  
  # select best model from each of the p models from AIC
  return(M_k_list)
} 

best_subset <- backwards.gbm(trainx, trainy, 1000, 2, 0.1)

```


### Feature selection
We fix a set of parameters and find the best subset of features given these parameter values, chosen with AIC.

```{r}

```


### Paramter tuning
Now we tune the parameters to the chosen subset of predictors with K-fold cross validation. 

```{r}

```

This is all like a greedy algorithm for finding the best model out of all possible models by reducing the search space



Now that we have the best boosted model out of all the 
### CV each method

### Result
```{r}
longitudinal.test.boost <- mutate(longitudinal.test, Demented = ifelse(Demented == "FALSE", 0,1))

yhat.boost <- predict(longitudinal.boost, newdata = longitudinal.test.boost, 
                      n.trees = 5000, type = "response")
yhat.boost <- ifelse(yhat.boost > 0.5, 1,0)
table(pred = yhat.boost, truth = longitudinal.test.boost$Demented) # 0.8648649

```



<!--  Tinus' initial work here. -->



## Introduction: Scope and purpose of your project


## Descriptive data analysis/statistics


## Methods


## Results and interpretation


## Summary
