---
title: "Compulsory Exercise 2: Prediction of Alzheimers Disease from Longitudinal MRI Data"
author:
- Chester Henry Charlton
- Kasper Eikeland
- Tinus Garshol
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: \usepackage{amsmath}
output:
  html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  #pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: "This is the place for your abstract (max 350 words)"
---
  
```{r setup, include=FALSE}
library("knitr")
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE, # nolint
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("rmarkdown")
library("dplyr")
library("GGally")
library("tidyr")
library("formatR")
```

```{r}
#setwd("/Users/chcharlton/NTNU/statLearning/compulsory_alzheimers")
#setwd("/Users/kaspe/Documents/NTNU/23V/TMA4268_Statistical_Learning/Projects/tma4268_alzheimers")
setwd("C:/Users/tinus/OneDrive/Desktop/GitHub/tma4268_alzheimers")
```


```{r}
cross_sectional.df <- read.csv("oasis_cross-sectional.csv")
# Patient is demented if CDR is > 0
cs.mutated <- cross_sectional.df %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, CDR)) %>%
  na.omit() %>%
  mutate(Demented = as.factor(CDR>0),
         M.F = as.factor(M.F)) %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, Demented))
longitudinal.df <- read.csv("oasis_longitudinal.csv")
longitudinal_deltas <- longitudinal.df %>% 
  group_by(Subject.ID) %>%
  summarise(dMMSE = (last(MMSE) - first(MMSE)) / last(Visit), 
            deTIV = (last(eTIV) - first(eTIV)) / last(Visit),
            dnWBV = (last(nWBV) - first(nWBV)) / last(Visit),
            dASF = (last(ASF) - first(ASF)) / last(Visit),
            dAge = (last(Age) - first(Age)) / last(Visit))
baseline_with_deltas <- longitudinal.df %>%
  filter(Visit == 1) %>%
  left_join(longitudinal_deltas, by = c("Subject.ID")) %>%
  mutate(Demented = as.factor(Group != "Nondemented"), # Those who converted became demented
         M.F = as.factor(M.F)) %>% 
  dplyr::select(-c(Subject.ID, MRI.ID, Group, Visit, MR.Delay, Hand, CDR, SES)) 
  
set.seed(1337)
# Longitudinal data
l.train.indeces <- sample(1:nrow(baseline_with_deltas), floor(.75 * nrow(baseline_with_deltas)))
longitudinal.train <- na.omit(baseline_with_deltas[l.train.indeces,])
longitudinal.test <- na.omit(baseline_with_deltas[-l.train.indeces,])
# Cross-sectional
cs.train.indeces <- sample(1:nrow(cs.mutated), floor(.75 * nrow(cs.mutated)))
cs.train <- na.omit(cs.mutated[cs.train.indeces,])
cs.test <- na.omit(cs.mutated[-cs.train.indeces,])
```
```

```{r}
library("tree")
library("gbm")
library("randomForest")


set.seed(1337)
# Longitudinal data
train <- sample(1:nrow(baseline_with_deltas), floor(.75 * nrow(baseline_with_deltas)))
longitudinal.train <- na.omit(baseline_with_deltas[train,])
longitudinal.test <- na.omit(baseline_with_deltas[-train,])
```


```{r}
# Classification tree
set.seed(1337)
longitudinal.tree <- tree(Demented ~., longitudinal.train)
summary(longitudinal.tree)
plot(longitudinal.tree)
text(longitudinal.tree, pretty = 0)

# Test predictions
pred.tree <- predict(longitudinal.tree, newdata=longitudinal.test, type = "class")
table(pred = pred.tree, truth = longitudinal.test$Demented) # 0.7837838
# Pruning?
```


```{r}
# Bagging
set.seed(1337)
longitudinal.bag <- randomForest(Demented ~., longitudinal.train, mtry = 12, importance = TRUE)
summary(longitudinal.bag)
plot(longitudinal.bag)
yhat.bag <- predict(longitudinal.bag, newdata = longitudinal.test, type = "class")
table(pred = yhat.bag, truth = longitudinal.test$Demented) # 0.8648649
```


```{r}
# Random forest
set.seed(1337)
longitudinal.rf <- randomForest(Demented ~., longitudinal.train, importance = TRUE)
summary(longitudinal.rf)
plot(longitudinal.rf)
yhat.rf <- predict(longitudinal.rf, newdata = longitudinal.test, type = "class")
table(pred = yhat.rf, truth = longitudinal.test$Demented) # 0.8108108
```


```{r}
# Boosting
set.seed(1337)
longitudinal.train.boost <- mutate(longitudinal.train, Demented = ifelse(Demented == "FALSE", 0,1))
longitudinal.boost <- gbm(Demented ~., data = longitudinal.train.boost, 
                          distribution = "bernoulli", n.trees = 5000, interaction.depth = 2)
summary(longitudinal.boost)
longitudinal.test.boost <- mutate(longitudinal.test, Demented = ifelse(Demented == "FALSE", 0,1))
<<<<<<< Updated upstream
yhat.boost <- predict(longitudinal.boost, newdata = longitudinal.test.boost, n.trees = 5000, type = "response")
yhat.boost <- ifelse(yhat.boost > 0.5, 1,0)
table(pred = yhat.boost, truth = longitudinal.test.boost$Demented) # 0.8648649
=======
yhat.boost <- predict(longitudinal.boost, newdata = longitudinal.test.boost, n.trees = 1000, type = "response")
yhat.boost <- ifelse(yhat.boost > 0.5, 1,0) # nolint
table(yhat.boost, longitudinal.test.boost$Demented)
>>>>>>> Stashed changes
```


```{r}
# Logistic
longitudinal.logistic <- glm(Demented ~., family = binomial, data = longitudinal.train)
logistic.probs <- predict(longitudinal.logistic, newdata = longitudinal.test, type = "response")
yhat.logistic <- ifelse(logistic.probs > .5, 1,0)
table(pred = yhat.logistic, truth = longitudinal.test$Demented) # 0.8648649

```


```{r}
# SVM
# Topics to investigate: 
# - subset selection?
# - parameter tuning: C, gamma and kernel
# - data with and without deltas

install.packages("e1071")
install.packages("caret")
library("e1071")
library("caret")

set.seed(58008)
# Linear kernel
lin.svm <- svm(Demented~.,data=longitudinal.train,kernel="linear",cost=10)
lin.svm.predict <- predict(lin.svm,newdata=longitudinal.test,type="class")
table(lin.svm.predict,longitudinal.test$Demented)

# Polynomial kernel
pol.svm <-svm(Demented~.,data=longitudinal.train,kernel="polynomial",cost=10)
pol.svm.predict <- predict(pol.svm,newdata=longitudinal.test,type="class")
table(pol.svm.predict,longitudinal.test$Demented)

# Radial kernel
rad.svm <-svm(Demented~.,data=longitudinal.train,kernel="radial",cost=10)
rad.svm.predict <- predict(rad.svm,newdata=longitudinal.test,type="class")
table(rad.svm.predict,longitudinal.test$Demented)

# Sigmoid kernel
sig.svm <-svm(Demented~.,data=longitudinal.train,kernel="sigmoid",cost=10)
sig.svm.predict <- predict(sig.svm,newdata=longitudinal.test,type="class")
table(sig.svm.predict,longitudinal.test$Demented)

# Testing for different costs "C" (longitudinal data)
set.seed(58008)
# Range of costs from 0-10
C = c(1:20)
best_cost = c()
# We'll do multiple passes with k-fold CV using the training data. 
# Then choose the average C
for (j in 1:10) {
  accuracy = c()
  for (i in 1:length(C)) {
    long.svm <- svm(Demented~.,data=longitudinal.train,kernel="linear",cost=C[i],cross=5)
    accuracy <- c(accuracy,long.svm[29])
  }
  index <- which.max(accuracy)
  best_cost <- c(best_cost,C[index])
}
print(best_cost)
# Result seems arbitrary
# choose the average C of the 'best ones'
avg_C = mean(best_cost)
long.svm <- svm(Demented~.,data=longitudinal.train,kernel="linear",cost=avg_C)
long.predict.svm <- predict(long.svm, newdata=longitudinal.test,type="class")
confusionMatrix(long.predict.svm,longitudinal.test$Demented)

# Testing with different "gammas"
set.seed(58008)
G = c(1:20)/20
accuracy = c()
for (i in 1:length(G)) {
  long.svm <- svm(Demented~.,data=longitudinal.train,kernel="polynomial",gamma=G[i],cross=10)
  accuracy <- c(accuracy,long.svm[29])
}
plot(G,accuracy)
index <- which.max(accuracy)
Gamma <- G[index]
print(Gamma)
long.svm <- svm(Demented~.,data=longitudinal.train,kernel="polynomial",gamma=Gamma)
long.predict.svm <- predict(long.svm, newdata=longitudinal.test,type="class")
confusionMatrix(long.predict.svm,longitudinal.test$Demented)

set.seed(58008)
# Dataframes with only deltas (longitudinal data)
deltas.long <- baseline_with_deltas %>% na.omit() %>% select(c(dAge,dASF,deTIV,dMMSE,dnWBV,Demented)) 
deltas.index <- sample(1:nrow(deltas.long), floor(.75 * nrow(deltas.long)))
deltas.train <- deltas.long[deltas.index,]
deltas.test <- deltas.long[-deltas.index,]

deltas.svm <- svm(Demented~., data=deltas.train,kernel="linear",cost=10)
deltas.predict <- predict(deltas.svm, newdata=deltas.test,type="class")
table(deltas.predict, deltas.test$Demented)
# Result is horrible, no matter the kernel

set.seed(58008)
# Dataframes without deltas (longitudinal data)
nodelt.long <- baseline_with_deltas %>% select(c(Age,ASF,eTIV,MMSE,nWBV,Demented)) %>% na.omit()
nodelt.index <- sample(1:nrow(nodelt.long), floor(.75 * nrow(nodelt.long)))
nodelt.train <- nodelt.long[nodelt.index,]
nodelt.test <- nodelt.long[-nodelt.index,]

nodelt.svm <- svm(Demented~., data=nodelt.train,kernel="linear",cost=10)
nodelt.predict <- predict(nodelt.svm, newdata=nodelt.test,type="class")
table(nodelt.predict, nodelt.test$Demented)
# Result is still worse without deltas

confusionMatrix(nodelt.predict, nodelt.test$Demented)

set.seed(58008)
SVM <- svm(Demented~.,data=longitudinal.train,kernel="linear",cross=10)
print(SVM[29])

```


```{r}
# LDA
library("MASS")

longitudinal.lda <- lda(Demented ~ ., data = longitudinal.train)
yhat.lda <- predict(longitudinal.lda, newdata = longitudinal.test)$class
table(pred = yhat.lda, truth = longitudinal.test$Demented) # 0.8108108

```


```{r}
# QDA
longitudinal.qda <- qda(Demented ~ ., data = longitudinal.train)
yhat.qda <- predict(longitudinal.qda, newdata = longitudinal.test)$class
table(pred = yhat.qda, truth = longitudinal.test$Demented) # 0.8648649
```



<!--  Tinus' initial work here. -->



## Introduction: Scope and purpose of your project


## Descriptive data analysis/statistics
```{r}

```

## Methods


## Results and interpretation


## Summary
