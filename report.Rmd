---
title: "Compulsory Exercise 2: Prediction of Alzheimers Disease from Longitudinal MRI Data"
author:
- Chester Henry Charlton
- Kasper Eikeland
- Tinus Garshol
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: \usepackage{amsmath}
output:
  html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  #pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: "This is the place for your abstract (max 350 words)"
---
  
```{r setup, include=FALSE}
library("knitr")
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE, 
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("rmarkdown")
library("dplyr")
library("GGally")
library("tidyr")
library("formatR")
```

```{r}
#setwd("/Users/chcharlton/NTNU/statLearning/compulsory_alzheimers")
#setwd("/Users/kaspe/Documents/NTNU/23V/TMA4268_Statistical_Learning/Projects/tma4268_alzheimers")
setwd("C:/Users/tinus/OneDrive/Desktop/GitHub/tma4268_alzheimers")
```

Data pre-processing
```{r}
cross_sectional.df <- read.csv("oasis_cross-sectional.csv")
# Patient is demented if CDR is > 0
cs.mutated <- cross_sectional.df %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, CDR)) %>%
  na.omit() %>%
  mutate(Demented = as.factor(CDR>0),
         M.F = as.factor(M.F)) %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, Demented))
longitudinal.df <- read.csv("oasis_longitudinal.csv")
longitudinal_deltas <- longitudinal.df %>% 
  group_by(Subject.ID) %>%
  summarise(dMMSE = (last(MMSE) - first(MMSE)) / last(Visit), 
            deTIV = (last(eTIV) - first(eTIV)) / last(Visit),
            dnWBV = (last(nWBV) - first(nWBV)) / last(Visit),
            dASF = (last(ASF) - first(ASF)) / last(Visit),
            dAge = (last(Age) - first(Age)) / last(Visit))
baseline_with_deltas <- longitudinal.df %>%
  filter(Visit == 1) %>%
  left_join(longitudinal_deltas, by = c("Subject.ID")) %>%
  mutate(Demented = as.factor(Group != "Nondemented"), # Those who converted became demented
         M.F = as.factor(M.F)) %>% 
  dplyr::select(-c(Subject.ID, MRI.ID, Group, Visit, MR.Delay, Hand, CDR, SES)) 
  
set.seed(1337)
# Longitudinal data
l.train.indeces <- sample(1:nrow(baseline_with_deltas), floor(.75 * nrow(baseline_with_deltas)))
longitudinal.train <- na.omit(baseline_with_deltas[l.train.indeces,])
longitudinal.test <- na.omit(baseline_with_deltas[-l.train.indeces,])
# Cross-sectional
cs.train.indeces <- sample(1:nrow(cs.mutated), floor(.75 * nrow(cs.mutated)))
cs.train <- na.omit(cs.mutated[cs.train.indeces,])
cs.test <- na.omit(cs.mutated[-cs.train.indeces,])
```

# SVM
```{r}
# Necessary packages
library("e1071")
library("caret")
```
Support vector machines creates hyperplanes to seperate the feature space and is commonly used in classification tasks. 
The core idea is that the method finds the "best" seperating hyperplane given a set of points,
usually by solving a complex minimization task using kernel functions. 
The math involved is beyond the scope of this project. 

The "e1071" package in R has a built-in svm function. 
It can also perform k-fold CV on the data which returns an accuracy metric - perecentage of predictions it got right given a set of data.
When using SVM's, the first step is to standardize data, which usually yields better performance when fitting models (common practice).
(might be that the svm function does this internally)
```{r}
summary(longitudinal.train)
st.long.train <- longitudinal.train
# Standardize/normalize all the numeric features
st.long.train[,2:12] <- scale(st.long.train[,2:12]) 
summary(st.long.train)
```
The second step is to make a choice of kernel function. 
The svm-function has four options of kernels: Linear, Polynomial, Radial and Sigmoid.
Usually, as the feature space increases data tends to become more linearly seperable. 
So a linear kernel is a natural choice to test.
Polynomial kernels increase rapidly in complexity as dimensions increase. 
Also, there is the issue of which degree of polynomial one wants to choose. 
Larger degree polynomial hyperplanes quickly result in overfitting of data. 
Thus, for the sake of simplicity we will avoid testing this kernel.
The RBF-kernel, or the radial basis function kernel is widely used in practice 
and often yields good results, so it would be interesting to compare with this one. 
The Sigmoid kernel is usually good for binary classification problems, but is avoided
in this investigation. In pre-testing, the results did not seem promising for this kernel,
so it will be avoided here. Altough, it would be interesting to investigate it further
in a more detailed SVM analysis.

We run an initial test using the linear and radial kernel's under default parameters and test their accuracy on the training set with 10-fold CV.
```{r}
set.seed(58008)
# Linear kernel
lin.svm <- svm(Demented~.,data=st.long.train,kernel="linear",cross=10)
summary(lin.svm)

# Radial kernel
rad.svm <-svm(Demented~.,data=st.long.train,kernel="radial",cross=10)
summary(rad.svm)
```
Both kernels perform decent, with accuracy > 75%. The linear kernel performs slightly better
under deafult choice of parameters. 

Now onto investigating tuning parameters.
Beginning with tuning the cost-parameter C. C acts as a weight of how hard we penalize
data that lies in the margin of the boundary seperating the prediction. 
Small C's result in large margins, but can lead to underfitting. 
Large C's result in small margins, but can lead to overfitting. 

C is the only tuning parameter for a linear kernel, so we start there.
Here we will perform 15 iterations testing values of C in [10^-4,10^3] (by orders of magnitude).
For each run, we will choose the best C. 
```{r}
set.seed(58008)
# Testing accuracy for different costs "C" 
# We'll do multiple passes with k-fold CV using the training data. 
# Range of costs, by orders of magnitude (C=10^4 results in max iteration problems)

# Linear kernel
C = 10^(-4:3)
best_cost_lin = c()
for (j in 1:15) {
  accuracy = c()
  for (i in 1:length(C)) {
    lin.svm <- svm(Demented~.,data=st.long.train,kernel="linear",cost=C[i],cross=10,scale=FALSE)
    accuracy <- c(accuracy,lin.svm[29])
  }
  index <- which.max(accuracy)
  best_cost_lin <- c(best_cost_lin,C[index])
}

tab <- table(best_cost_lin)
y <- names(tab)
x <- as.numeric(tab)
barplot(x, names.arg = y, ylab = "Frequency", xlab = "Cost")
``` 
A cost of 1 seems to be most prevalent after 15 runs.

A SVM with a radial kernel also has a tuning parameter gamma. Gamma is closely
related to the curvature of the seperating boundary, or the "stiffness" of it.
Here we need to test with both gamma and cost simultaneously as these parameters are correlated.
The e1071 library has a built-in tune function which performs this analysis.
We'll do 10 iterations, with gamma in [10^-4,10^3] and cost in [10^-4,10^3]. For each iteration, we choose the best pair.
```{r}
set.seed(58008)
# 10 runs with 10-fold CV. Selecting the best combination of gamma and C
# in each iteration.
gammas <- c()
costs <- c()
for (i in 1:10) {
  obj <- tune(svm, Demented~., data = st.long.train,ranges = list(gamma = 10^(-4:3), cost = 10^(-4:3)),tunecontrol = tune.control(cross=10))
  GC <- summary(obj)[1]
  best_cost <- GC[[1]]$cost
  best_gamma <- GC[[1]]$gamma
  gammas <- c(gammas,best_gamma)
  costs <- c(costs,best_cost)
}
tab <- table(gammas,costs)
print(tab)
```
From 10 runs, a value of gamma = 0.01 and cost = 100 produced the best results 6 times.

With the paramteres tuned for both kernels, we test their performance against one another.
```{r}
set.seed(58008)
# Linear kernel, cost = 1
lin.svm <- svm(Demented~.,data=st.long.train,kernel="linear",cost=1,cross=10)
lin_accuracy <- lin.svm[29]
print(lin_accuracy)
# Radial kernel, gamma = 0.01, cost = 100
radial.svm <- svm(Demented~.,data=st.long.train,kernel="radial",gamma=1e-02,cost=1e+02,cross=10)
rad_accuracy <- radial.svm[29]
print(rad_accuracy)
```
Both models have an accuracy of approx. 80%, with the radial kernel outperforming the linear.
The SVM with linear kernel remains unaffected, as a cost of 1 was the default paramter we began with.
The SVM with radial kernel has approved in accuracy by approx 5% through tuning.
(I wouldn't say the results are conclusive in determining which model is best)

(I think it would be wise to omit this section, can't reproduce results and it'll just clutter the project)
This section is about how the SVM performs when only considering deltas/removing deltas.
First we need to create two new training sets from the data set. One using categorical variables and deltas, 
and one using categorical variables without deltas.
```{r}
# Dataframes with only deltas
set.seed(58008)
deltas.long <- baseline_with_deltas %>% na.omit() %>% select(c(M.F,dAge,dASF,deTIV,dMMSE,dnWBV,Demented)) 
deltas.index <- sample(1:nrow(deltas.long), floor(.75 * nrow(deltas.long)))
deltas.train <- deltas.long[deltas.index,]

# Dataframes without deltas
nodelt.long <- baseline_with_deltas %>% select(c(M.F,Age,ASF,eTIV,MMSE,nWBV,Demented)) %>% na.omit()
nodelt.index <- sample(1:nrow(nodelt.long), floor(.75 * nrow(nodelt.long)))
nodelt.train <- nodelt.long[nodelt.index,]

# Normalize new training sets
deltas.train[,2:6] <- scale(deltas.train[,2:6]) 
nodelt.train[,2:6] <- scale(nodelt.train[,2:6])
```

We'll do one SVM call for each training set, using default parameters.
```{r}
set.seed(58008)

# Default parameters: RBF-kernel, C = 1, gamma = 1/(no. of features), C-classification

# Data with only categorical variables and deltas
deltas.svm <- svm(Demented~., data=deltas.train,cross=10)
print("Data with only deltas: ")
print(deltas.svm[29])

# Data with only categorical variables and without deltas
nodelt.svm <- svm(Demented~., data=nodelt.train,cross=10)
print("Data without deltas")
print(nodelt.svm[29])
``` 

# Summary
