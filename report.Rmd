---
title: "Compulsory Exercise 2: Prediction of Alzheimers Disease from Longitudinal MRI Data"
author:
- Chester Henry Charlton
- Kasper Eikeland
- Tinus Garshol
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: \usepackage{amsmath}
output:
  html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  #pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: "This is the place for your abstract (max 350 words)"
---
  
```{r setup, include=FALSE}
library("knitr")
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",fig.width=4, fig.height=3,fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("rmarkdown")
library("dplyr")
library("GGally")
library("tidyr")
library("formatR")
```

```{r}
#setwd("/Users/chcharlton/NTNU/statLearning/compulsory_alzheimers")
setwd("/Users/kaspe/Documents/NTNU/23V/TMA4268_Statistical_Learning/Projects/tma4268_alzheimers")
```


<!--  Henry's initial work here -->


<!--  Kasper's initial work here. -->
```{r}
cross_sectional.df <- read.csv("oasis_cross-sectional.csv")

# Patient is demented if CDR is > 0
cs.mutated <- cross_sectional.df %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, CDR)) %>%
  na.omit() %>%
  mutate(Demented = as.factor(CDR>0),
         M.F = as.factor(M.F)) %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, Demented))


longitudinal.df <- read.csv("oasis_longitudinal.csv")

longitudinal_deltas <- longitudinal.df %>% 
  group_by(Subject.ID) %>%
  summarise(dMMSE = (last(MMSE) - first(MMSE)) / last(Visit), 
            deTIV = (last(eTIV) - first(eTIV)) / last(Visit),
            dnWBV = (last(nWBV) - first(nWBV)) / last(Visit),
            dASF = (last(ASF) - first(ASF)) / last(Visit),
            dAge = (last(Age) - first(Age)) / last(Visit))

baseline_with_deltas <- longitudinal.df %>%
  filter(Visit == 1) %>%
  left_join(longitudinal_deltas, by = c("Subject.ID")) %>%
  mutate(Demented = as.factor(Group != "Nondemented"), # Those who converted became demented
         M.F = as.factor(M.F)) %>% 
  dplyr::select(-c(Subject.ID, MRI.ID, Group, Visit, MR.Delay, Hand, CDR, SES)) 
  
set.seed(1337)

# Longitudinal data
l.train.indeces <- sample(1:nrow(baseline_with_deltas), floor(.75 * nrow(baseline_with_deltas)))
longitudinal.train <- na.omit(baseline_with_deltas[l.train.indeces,])
longitudinal.test <- na.omit(baseline_with_deltas[-l.train.indeces,])

# Cross-sectional
cs.train.indeces <- sample(1:nrow(cs.mutated), floor(.75 * nrow(cs.mutated)))
cs.train <- na.omit(cs.mutated[cs.train.indeces,])
cs.test <- na.omit(cs.mutated[-cs.train.indeces,])
```

```{r}
library("tree")
library("gbm")
library("randomForest")
```


```{r}
# Boosting, longitudinal
set.seed(1337)

# gbm wants response in {0,1}
longitudinal.train.boost <- mutate(longitudinal.train, Demented = ifelse(Demented == "FALSE", 0,1))
```

### Boosted classification tree

Here we a classification tree by boosting. This is done using the ´gbm´ package. To choose the number of trees to use we employ 10-fold cross-validation though the function itself. To choose the interaction depth we examine four different models and choose the model with lowest overall cross-validation error. We use a shrinkage of 0.1 for all models. 

```{r}
gbm1 <- gbm(Demented ~., data = longitudinal.train.boost, distribution = "bernoulli" ,n.trees = 5000, interaction.depth = 1, shrinkage = 0.1, cv.folds = 10, n.cores = 6)
gbm2 <- gbm(Demented ~., data = longitudinal.train.boost, distribution = "bernoulli" ,n.trees = 5000, interaction.depth = 2, shrinkage = 0.1, cv.folds = 10, n.cores = 6)
gbm3 <- gbm(Demented ~., data = longitudinal.train.boost, distribution = "bernoulli" ,n.trees = 5000, interaction.depth = 3, shrinkage = 0.1, cv.folds = 10, n.cores = 6)
gbm4 <- gbm(Demented ~., data = longitudinal.train.boost, distribution = "bernoulli" ,n.trees = 5000, interaction.depth = 4, shrinkage = 0.1, cv.folds = 10, n.cores = 6)

iters = 1:100

cv.df <- data.frame(iteration = iters, 
                    cv1 = gbm1$cv.error[iters], cv2 = gbm2$cv.error[iters], 
                    cv3 = gbm3$cv.error[iters], cv4 = gbm4$cv.error[iters])

ggplot(cv.df, aes(x=iteration)) + 
  geom_line(aes(y = cv1, color = "d=1")) + 
  geom_line(aes(y = cv2, color = "d=2")) +
  geom_line(aes(y = cv3, color = "d=3")) +
  geom_line(aes(y = cv4, color = "d=4")) +
  geom_point(aes(x = which.min(cv1), y = min(cv1))) +
  geom_point(aes(x = which.min(cv2), y = min(cv2))) +
  geom_point(aes(x = which.min(cv3), y = min(cv3))) +
  geom_point(aes(x = which.min(cv4), y = min(cv4))) +
  labs(x = "Iteration", y = "Cross-validation error") + 
  theme_minimal()
```

From the above plot we see that an interaction depth of 2 gives the minimum CV-error with `{r} which.min(cv.df$cv2)` iterations. Thus we use an interaction depth of 2 with 22 trees and a shrinkage of 0.1 for the boosted classification tree. 




### Feature selection sensitivity test
Quick test to see if the feature selection is sensitive to the choice of parameters.
```{r}
# Feature selection sensitivity test
library("stepgbm")

trainx <- select(longitudinal.train.boost, -Demented)
trainy <- longitudinal.train.boost$Demented

# backwards feature selection
backwards.gbm <- function(trainx, trainy, B, d, shrinkage, folds) {
  
  # Step 1
  M_k_list <- data.frame(predictors = rep(0, ncol(trainx)), misclass_rate = rep(0, ncol(trainx)), cv_err = rep(0,ncol(trainx)), iterations = rep(0,ncol(trainx)))
  M_k_list$predictors[ncol(trainx)] <- list(1:ncol(trainx))
  predictors_k <- c(1:ncol(trainx)) # start with all predictors
  
  # Step 2
  # Consider all k models that contain all but one of the predictors in M_k for a total of k-1 predictors

  for (k in ncol(trainx):d) { # iterate backwards, minimum d predictors
  
    results_k.df <- data.frame(predictors = I(rep(list(rep(0,k-1)), k)), 
                               misclass_rate = rep(0, k), iterations = rep(0, k)) # k rows, 3 columns, first column is which predictors
    
    for (j in 1:k) { # iterate through predictors_k
      current_predictors <- predictors_k[-j]  # exclude jth predictor
      
      M_k_tmp <- gbm(trainy ~ ., data = select(trainx, current_predictors),
                     distribution = "bernoulli", n.trees = B, interaction.depth = d, 
                     shrinkage = shrinkage, cv.folds = 5) # fit model with current selection of predictors
      
      results_k.df$misclass_rate[j] <- min(M_k_tmp$train.error) # save training error from gbm object
      results_k.df$predictors[j] <- list(current_predictors) # save corresponding predictors
      results_k.df$iterations[j] <- gbm.perf(M_k_tmp, plot.it = FALSE, method = "cv") # save number of trees
      
      }
    
    best.model_k <- which.min(results_k.df$misclass_rate)# select model with minimum training loss - should we use a different measure?
    
    predictors_k <- unlist(results_k.df$predictors[best.model_k]) # get predictors from model M_k, preferably vector of indeces
    
    M_k_list$predictors[k-1] <- list(predictors_k) # save which predictors give min misclass rate for k predictors
    M_k_list$misclass_rate[k-1] <- results_k.df$misclass_rate[best.model_k]
    M_k_list$iterations <- results_k.df$iterations[best.model_k]
  }
  
  # Takes a lot of compute to run CV on all models when we're only intereseted in the ones we select for each k
  # for (k in 2:ncol(trainx)){ 
  #   predictors <- unlist(M_k_list$predictors[k])
  #   model <- gbm(trainy ~ ., data = select(trainx, all_of(predictors)),
  #                   distribution = "bernoulli", n.trees = M_k_list$iterations[k], interaction.depth = d, shrinkage = shrinkage, cv.folds = folds, n.cores = 2) # refit models
  #   M_k_list$cv_err[k] <- model$cv.error # save cv error
  #  }
  #return(M_k_list$predictors[which.min(M_k_list$cv_err)]) # return predictors of model with lowest cv error
  return(M_k_list)

  } 

set.seed(1337)
best_subset1 <- backwards.gbm(trainx, trainy, 200, 2, 0.1, 5)

```




### Feature selection
We fix a set of parameters and find the best subset of features given these parameter values, chosen with AIC.

```{r}

```


### Paramter tuning
Now we tune the parameters to the chosen subset of predictors with K-fold cross validation. 

```{r}

```

This is all like a greedy algorithm for finding the best model out of all possible models by reducing the search space



Now that we have the best boosted model out of all the 
### CV each method

### Result
```{r}
longitudinal.test.boost <- mutate(longitudinal.test, Demented = ifelse(Demented == "FALSE", 0,1))

yhat.boost <- predict(longitudinal.boost, newdata = longitudinal.test.boost, 
                      n.trees = 5000, type = "response")
yhat.boost <- ifelse(yhat.boost > 0.5, 1,0)
table(pred = yhat.boost, truth = longitudinal.test.boost$Demented) # 0.8648649

```



<!--  Tinus' initial work here. -->



## Introduction: Scope and purpose of your project


## Descriptive data analysis/statistics


## Methods


## Results and interpretation


## Summary
