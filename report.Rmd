---
title: "Compulsory Exercise 2: Prediction of Alzheimers Disease from Longitudinal MRI Data"
author:
- Chester Henry Charlton
- Kasper Eikeland
- Tinus Garshol
date: "`r format(Sys.time(), '%d %B, %Y')`"
header-includes: \usepackage{amsmath}
output:
  html_document:
  #   toc: no
  #   toc_depth: '2'
  #   df_print: paged
  #pdf_document:
    toc: no
    toc_depth: '2'
urlcolor: blue
abstract: "While Alzheimer's disease is traditionally diagnosed by medical professionals, in this report we detail the use of statistical learning models to classify Alzheimer's disease. Cross-sectional and Longitudinal MRI data was used from a mix of nondemented and demented patients. This data also included several non-MRI features, such as assessments by medical professionals, education level, and the final diagnosis, which was used as a target variable for the models. The primary models tested were decision trees(?) using the random forest method, support vector machines, and logistic regression. Support vector machines and decision trees were chosen based on applicability, and logistic regression was chosen due to its relevance and consistent use in the mdeical field. [sentence describing results - we don't have this yet]. While the significance of this report was lessened in ambition by the lack of data for patients who were converted from nondemented to demented, it still holds value as a way for screening patients without the assistance of a doctor, and in the more explainable models provides a way to determine the relative importance of variables in relation to Alzheimer's disease. Furthermore, by applying our models both to the corss-sectional and longitudinal data, we can make some conclusions about the efficacy in temporal data in classifying Alzheimer's disease."
---

```{r setup, include=FALSE}
library("knitr")
knitr::opts_chunk$set(echo=TRUE, eval=TRUE, message=FALSE, warning = FALSE, 
                      strip.white = TRUE, prompt = FALSE, cache = TRUE, 
                      size = "scriptsize", fig.width = 4, fig.height = 3,
                      fig.align = "center")

```

```{r,eval=TRUE,echo=FALSE}
library("rmarkdown")
library("dplyr")
library("GGally")
library("tidyr")
library("formatR")
library('DiagrammeR')
library('bestglm')
```

## The block below is specific to Henry's Computer
```{r}
#specific to Henry's computer
setwd("/Users/chcharlton/NTNU/statLearning/tma4268_alzheimers")
```

## The block below is universal, each groupmember will use it build from
```{r}
cross_sectional.df <- read.csv("oasis_cross-sectional.csv")
# Patient is demented if CDR is > 0
cs.mutated <- cross_sectional.df %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, CDR)) %>%
  na.omit() %>%
  mutate(Demented = as.factor(CDR>0),
         M.F = as.factor(M.F)) %>%
  dplyr::select(c(M.F, eTIV, MMSE, nWBV, ASF, Age, Demented))
longitudinal.df <- read.csv("oasis_longitudinal.csv")
longitudinal_deltas <- longitudinal.df %>% 
  group_by(Subject.ID) %>%
  summarise(dMMSE = (last(MMSE) - first(MMSE)) / last(Visit), 
            deTIV = (last(eTIV) - first(eTIV)) / last(Visit),
            dnWBV = (last(nWBV) - first(nWBV)) / last(Visit),
            dASF = (last(ASF) - first(ASF)) / last(Visit),
            dAge = (last(Age) - first(Age)) / last(Visit))
baseline_with_deltas <- longitudinal.df %>%
  filter(Visit == 1) %>%
  left_join(longitudinal_deltas, by = c("Subject.ID")) %>%
  mutate(Demented = as.factor(Group != "Nondemented"), # Those who converted became demented
         M.F = as.factor(M.F)) %>% 
  dplyr::select(-c(Subject.ID, MRI.ID, Group, Visit, MR.Delay, Hand, CDR, SES)) 
  
set.seed(1337)
# Longitudinal data
l.train.indices <- sample(1:nrow(baseline_with_deltas), floor(.75 * nrow(baseline_with_deltas)))
longitudinal.train <- na.omit(baseline_with_deltas[l.train.indices,])
longitudinal.test <- na.omit(baseline_with_deltas[-l.train.indices,])
# Cross-sectional
cs.train.indices <- sample(1:nrow(cs.mutated), floor(.75 * nrow(cs.mutated)))
cs.train <- na.omit(cs.mutated[cs.train.indices,])
cs.test <- na.omit(cs.mutated[-cs.train.indices,])
```

## Logistic regression

```{r}
# model creation on training set
# logistic regression on longitudinal data
longitudinal.logistic <-  glm(Demented ~ . - dMMSE - MMSE, data = longitudinal.train, family = binomial)
longitudinal.logistic.probs <- predict(longitudinal.logistic, newdata = longitudinal.test, type = "response")
longitudinal.logistic.pred <- ifelse(longitudinal.logistic.probs > 0.5, 1, 0)

longitudinal.logistic.confusion <- table(longitudinal.logistic.pred, longitudinal.test$Demented)
rownames(longitudinal.logistic.confusion) <- c('pred 0','pred 1')
colnames(longitudinal.logistic.confusion) <- c('true 0','true 1')

longitudinal.logistic.sens <- longitudinal.logistic.confusion[2,2] / sum(longitudinal.logistic.confusion[,2])
longitudinal.logistic.spec <- longitudinal.logistic.confusion[1,1] / sum(longitudinal.logistic.confusion[,1])
longitudinal.logistic.vals = c(longitudinal.logistic.sens, longitudinal.logistic.spec)

longitudinal.logistic.confusion
longitudinal.logistic.vals

```
longitudinal logistic regression on all variables: 

longitudinal.logistic.pred true 0 true 1
                    pred 0     15      4
                    pred 1      1     17
[1] 0.8095238 0.9375000
     (sens)     (spec)
 
longitudinal logistic regression on all variables except deltas: 

longitudinal.logistic.pred true 0 true 1
                    pred 0     15      8
                    pred 1      1     13
[1] 0.6190476 0.9375000
     (sens)    (spec)




## Model selection



### using bestglm and just BIC

```{r}
# longitudinal subset selection
longitudinal.best.bic <- bestglm(longitudinal.train, family = binomial, IC = "BIC")
longitudinal.best.aic <- bestglm(longitudinal.train, family = binomial, IC = "AIC", method ="forward")

# get residuals for best model
longitudinal.best.bic.errors <- resid(longitudinal.best.bic$BestModel)
longitudinal.best.aic.errors <- resid(longitudinal.best.aic$BestModel)

# get subset tables (for a given number of variables, which ones are best)
longitudinal.bic.subsets <- 
  longitudinal.best.bic$Subsets %>% 
  data.frame() %>% 
  cbind(variables = 1:nrow(longitudinal.bic.subsets))
  
longitudinal.aic.subsets <- 
  longitudinal.best.aic$Subsets %>% 
  data.frame() %>% 
  cbind(variables = 1:nrow(longitudinal.aic.subsets))

qqnorm(longitudinal.best.bic.errors, ylab = "Longitudinal Best BIC Residuals")
qqnorm(longitudinal.best.aic.errors, ylab = "Longitudinal Best AIC Residuals")


```

```{r}

longitudinal.bic.subsets.plot <- 
  ggplot(longitudinal.bic.subsets, aes(x = variables, y = BIC)) +
  geom_point(color = "hotpink", shape = 'X', size = 4) +
  ggtitle("Longitudinal BIC Subset Selection") +
  labs(x = "Variables", y = "BIC") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  theme_minimal() 


longitudinal.aic.subsets.plot <- 
  ggplot(longitudinal.aic.subsets, aes(x = variables, y = AIC)) +
  geom_point(color = "chartreuse4", shape = "X", size = 4) +
  ggtitle("Longitudinal AIC Subset Selection") +
  labs(x = "Variables", y = "AIC") +
  scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
  scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
  theme_minimal() 
```


## plotting the master-diagram

```{r}
mermaid("
graph TD
    start[source data]
    process[processed data]
    start --> process[processed data]
    process --> a0
    process --> a1


    a0 -->  b0 
    a0 -->  b1 
    a0 -->  b2
    a0[training data]
    a1[test data]
    b0[SVM]
    b1[Boosting]
    b2[Logistic]
    c0[subsetted SVM]
    c1[subsetted Boosting]
    
    d0[best SVM]
    d1[best Boosting]
    d2[best Logistic]
    
    e0[best model selection]
    
    b0 --> |feat param sensitivity| c0
    b1 --> |feat param sensitivity| c1
    b2 --> |feature selection| d2
    
    c0 --> |parameter tuning| d0
    c1 --> |parameter tuning| d1
    



    a1 --> e0
    d0 --> e0
    d1 --> e0
    d2 --> e0
    
    e0 --> final[interpretation]
    

    
")
```

```{r}
grViz("
digraph boxes_and_circles {

  # a 'graph' statement
  graph [overlap = true, fontsize = 10]

  # several 'node' statements
  node [shape = box,
        fontname = Helvetica]
  A; B; C; D; E; F

  node [shape = circle,
        fixedsize = true,
        width = 0.9] // sets as circles
  1; 2; 3; 4; 5; 6; 7; 8

  # several 'edge' statements
  A->1 B->2 B->3 B->4 C->A
  1->D E->A 2->4 1->5 1->F
  E->6 4->6 5->7 6->7 3->8
}
")
```

















## Introduction: Scope and purpose of your project


## Descriptive data analysis/statistics

### Description of Features:

Group: The group that the patient was classified in at the across the various scans. Possible values are "Undemented," denoting a patient without symptoms of dementia throughout all scans, "Demented" denoting a patient with symptoms of dementia across all scans, or "Converted," denoting someone who began without symptoms of dementia but developed them at a further scan.

Visit: The order of the visit in which the scan was done, indexed from 1.

MR Delay: The number of days since the previous scan. Marked as 0 for the initial scan.

M.F: Categorical variable for male or female patients at birth. M denotes male, F denotes female.

Hand: The patient's dominant hand. All of the patients were right-handed.

EDUC: The education level of the patient, measured in years (including elementary school).

SES: "Socioeconomic status as assessed by the Hollingshead Index of Social Position and classified into categories from 1 (highest status) to 5 (lowest status) (Hollingshead, 1957)"

MMSE: "Mini-Mental State Examination score (range is from 0 = worst to 30 = best) (Folstein, Folstein, & McHugh, 1975)"

CDR: "Clinical Dementia Rating (0 = no dementia, 0.5 = very mild AD, 1 = mild AD, 2 = moderate AD) (Morris, 1993)"

ASF: "Atlas scaling factor (unitless). Computed scaling factor that transforms native-space brain and skull to the atlas target (i.e., the determinant of the transform matrix) (Buckner et al., 2004)"

eTIV: "Estimated total intracranial volume (cm3) (Buckner et al., 2004)"

nWBV: "Normalized whole-brain volume, expressed as a percent of all voxels in the atlas-masked image that are labeled as gray or white matter by the automated tissue segmentation process (Fotenos et al., 2005)"



## Methods


## Results and interpretation

### points to discuss:

whether to include MMSE: Seems like it is objective, and only takes a few minutes to run. It could theoretically be run by existing NLP structures, so we could make an argument that using this does not compromise our goal of eliminating pressure on medical professionals. 

whether having an high specificity or sensitivity is more important: what is worse: (1) someone who truly does not have alzheimer's being told they do, or (2) someone who truly does have alzheimer's being told they don't? If (1): 


overall strategy like Kasper drew:
treat the entire process as model selection. separate test and training data. Set aside test data until we choose a final best model based on the training data.  train models on training data, setting an arbitrary set of parameters -> perform a rough sensitivity analysis for the effect of parameters on the feature selection -> choose arbitrary parameters -> perform feature selection, choosing best set of features -> tune parameters for the model with best feature set -> evaluate all three models with k-fold cv in parallel -> select best model -> 



## Summary












